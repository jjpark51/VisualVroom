{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset split sizes:\n",
      "Total dataset size: 3840\n",
      "Training set size: 2687 (70.0%)\n",
      "Validation set size: 385 (10.0%)\n",
      "Test set size: 768 (20.0%)\n",
      "\n",
      "Dataset class distribution:\n",
      "ambulance_L: 480 samples\n",
      "ambulance_R: 480 samples\n",
      "carhorns_L: 480 samples\n",
      "carhorns_R: 480 samples\n",
      "FireTruck_L: 480 samples\n",
      "FireTruck_R: 480 samples\n",
      "policecar_L: 480 samples\n",
      "policecar_R: 480 samples\n",
      "Starting training...\n",
      "Using device: cuda\n",
      "Epoch [1/20], Batch [10/168], Loss: 1.5206, Acc: 21.88%\n",
      "Epoch [1/20], Batch [20/168], Loss: 1.0574, Acc: 34.38%\n",
      "Epoch [1/20], Batch [30/168], Loss: 0.8901, Acc: 37.50%\n",
      "Epoch [1/20], Batch [40/168], Loss: 0.7260, Acc: 42.19%\n",
      "Epoch [1/20], Batch [50/168], Loss: 0.9116, Acc: 44.38%\n",
      "Epoch [1/20], Batch [60/168], Loss: 0.8251, Acc: 46.35%\n",
      "Epoch [1/20], Batch [70/168], Loss: 0.7618, Acc: 47.50%\n",
      "Epoch [1/20], Batch [80/168], Loss: 0.7926, Acc: 48.67%\n",
      "Epoch [1/20], Batch [90/168], Loss: 0.8101, Acc: 49.44%\n",
      "Epoch [1/20], Batch [100/168], Loss: 0.7696, Acc: 50.31%\n",
      "Epoch [1/20], Batch [110/168], Loss: 0.5954, Acc: 51.08%\n",
      "Epoch [1/20], Batch [120/168], Loss: 0.7108, Acc: 51.61%\n",
      "Epoch [1/20], Batch [130/168], Loss: 0.7933, Acc: 51.92%\n",
      "Epoch [1/20], Batch [140/168], Loss: 0.5211, Acc: 53.12%\n",
      "Epoch [1/20], Batch [150/168], Loss: 0.7219, Acc: 53.79%\n",
      "Epoch [1/20], Batch [160/168], Loss: 0.7460, Acc: 54.41%\n",
      "Epoch [1/20], Train Loss: 0.8726, Train Acc: 54.75%, Val Loss: 0.6341, Val Acc: 65.19%, Time: 55.30s\n",
      "Epoch [2/20], Batch [10/168], Loss: 0.8651, Acc: 59.38%\n",
      "Epoch [2/20], Batch [20/168], Loss: 0.6539, Acc: 64.06%\n",
      "Epoch [2/20], Batch [30/168], Loss: 0.6122, Acc: 65.42%\n",
      "Epoch [2/20], Batch [40/168], Loss: 0.6337, Acc: 64.84%\n",
      "Epoch [2/20], Batch [50/168], Loss: 0.6967, Acc: 64.00%\n",
      "Epoch [2/20], Batch [60/168], Loss: 0.6687, Acc: 65.52%\n",
      "Epoch [2/20], Batch [70/168], Loss: 0.4173, Acc: 65.71%\n",
      "Epoch [2/20], Batch [80/168], Loss: 0.4114, Acc: 66.56%\n",
      "Epoch [2/20], Batch [90/168], Loss: 0.5404, Acc: 67.36%\n",
      "Epoch [2/20], Batch [100/168], Loss: 0.3938, Acc: 68.69%\n",
      "Epoch [2/20], Batch [110/168], Loss: 0.2867, Acc: 69.72%\n",
      "Epoch [2/20], Batch [120/168], Loss: 0.8728, Acc: 70.00%\n",
      "Epoch [2/20], Batch [130/168], Loss: 0.5817, Acc: 70.62%\n",
      "Epoch [2/20], Batch [140/168], Loss: 0.2008, Acc: 71.34%\n",
      "Epoch [2/20], Batch [150/168], Loss: 0.7361, Acc: 71.62%\n",
      "Epoch [2/20], Batch [160/168], Loss: 0.1727, Acc: 71.95%\n",
      "Epoch [2/20], Train Loss: 0.5375, Train Acc: 72.50%, Val Loss: 0.3766, Val Acc: 80.26%, Time: 55.70s\n",
      "Epoch [3/20], Batch [10/168], Loss: 0.4011, Acc: 81.88%\n",
      "Epoch [3/20], Batch [20/168], Loss: 0.2863, Acc: 81.88%\n",
      "Epoch [3/20], Batch [30/168], Loss: 0.3032, Acc: 82.92%\n",
      "Epoch [3/20], Batch [40/168], Loss: 0.1679, Acc: 83.91%\n",
      "Epoch [3/20], Batch [50/168], Loss: 0.7314, Acc: 84.62%\n",
      "Epoch [3/20], Batch [60/168], Loss: 0.6494, Acc: 84.17%\n",
      "Epoch [3/20], Batch [70/168], Loss: 0.4063, Acc: 82.68%\n",
      "Epoch [3/20], Batch [80/168], Loss: 0.6349, Acc: 81.80%\n",
      "Epoch [3/20], Batch [90/168], Loss: 0.4607, Acc: 81.39%\n",
      "Epoch [3/20], Batch [100/168], Loss: 0.4816, Acc: 82.19%\n",
      "Epoch [3/20], Batch [110/168], Loss: 0.2114, Acc: 82.39%\n",
      "Epoch [3/20], Batch [120/168], Loss: 0.1913, Acc: 82.19%\n",
      "Epoch [3/20], Batch [130/168], Loss: 0.2781, Acc: 82.16%\n",
      "Epoch [3/20], Batch [140/168], Loss: 0.5083, Acc: 82.77%\n",
      "Epoch [3/20], Batch [150/168], Loss: 0.3586, Acc: 82.96%\n",
      "Epoch [3/20], Batch [160/168], Loss: 0.1528, Acc: 83.20%\n",
      "Epoch [3/20], Train Loss: 0.3670, Train Acc: 83.62%, Val Loss: 0.4377, Val Acc: 85.71%, Time: 55.93s\n",
      "Epoch [4/20], Batch [10/168], Loss: 0.3377, Acc: 86.88%\n",
      "Epoch [4/20], Batch [20/168], Loss: 0.1422, Acc: 86.88%\n",
      "Epoch [4/20], Batch [30/168], Loss: 0.2087, Acc: 88.96%\n",
      "Epoch [4/20], Batch [40/168], Loss: 0.1817, Acc: 90.47%\n",
      "Epoch [4/20], Batch [50/168], Loss: 0.1466, Acc: 91.88%\n",
      "Epoch [4/20], Batch [60/168], Loss: 0.6395, Acc: 91.15%\n",
      "Epoch [4/20], Batch [70/168], Loss: 0.2373, Acc: 90.18%\n",
      "Epoch [4/20], Batch [80/168], Loss: 0.2994, Acc: 90.08%\n",
      "Epoch [4/20], Batch [90/168], Loss: 0.3748, Acc: 90.07%\n",
      "Epoch [4/20], Batch [100/168], Loss: 0.2739, Acc: 89.88%\n",
      "Epoch [4/20], Batch [110/168], Loss: 0.1553, Acc: 89.94%\n",
      "Epoch [4/20], Batch [120/168], Loss: 0.2897, Acc: 89.79%\n",
      "Epoch [4/20], Batch [130/168], Loss: 0.1790, Acc: 90.05%\n",
      "Epoch [4/20], Batch [140/168], Loss: 0.2489, Acc: 90.27%\n",
      "Epoch [4/20], Batch [150/168], Loss: 0.3726, Acc: 90.33%\n",
      "Epoch [4/20], Batch [160/168], Loss: 0.2019, Acc: 90.35%\n",
      "Epoch [4/20], Train Loss: 0.2264, Train Acc: 90.29%, Val Loss: 0.3552, Val Acc: 87.01%, Time: 56.09s\n",
      "Epoch [5/20], Batch [10/168], Loss: 0.2141, Acc: 91.88%\n",
      "Epoch [5/20], Batch [20/168], Loss: 0.1132, Acc: 89.69%\n",
      "Epoch [5/20], Batch [30/168], Loss: 0.1900, Acc: 89.17%\n",
      "Epoch [5/20], Batch [40/168], Loss: 0.5514, Acc: 87.66%\n",
      "Epoch [5/20], Batch [50/168], Loss: 0.2359, Acc: 87.38%\n",
      "Epoch [5/20], Batch [60/168], Loss: 0.1008, Acc: 88.85%\n",
      "Epoch [5/20], Batch [70/168], Loss: 0.1357, Acc: 89.29%\n",
      "Epoch [5/20], Batch [80/168], Loss: 0.1282, Acc: 90.16%\n",
      "Epoch [5/20], Batch [90/168], Loss: 0.1327, Acc: 90.76%\n",
      "Epoch [5/20], Batch [100/168], Loss: 0.0053, Acc: 91.31%\n",
      "Epoch [5/20], Batch [110/168], Loss: 0.2306, Acc: 91.31%\n",
      "Epoch [5/20], Batch [120/168], Loss: 0.3695, Acc: 91.15%\n",
      "Epoch [5/20], Batch [130/168], Loss: 0.3010, Acc: 90.87%\n",
      "Epoch [5/20], Batch [140/168], Loss: 0.1661, Acc: 90.62%\n",
      "Epoch [5/20], Batch [150/168], Loss: 0.0331, Acc: 90.92%\n",
      "Epoch [5/20], Batch [160/168], Loss: 0.0542, Acc: 91.09%\n",
      "Epoch [5/20], Train Loss: 0.2166, Train Acc: 91.29%, Val Loss: 0.1758, Val Acc: 91.95%, Time: 56.16s\n",
      "Epoch [6/20], Batch [10/168], Loss: 0.1823, Acc: 94.38%\n",
      "Epoch [6/20], Batch [20/168], Loss: 0.1327, Acc: 94.06%\n",
      "Epoch [6/20], Batch [30/168], Loss: 0.3893, Acc: 93.33%\n",
      "Epoch [6/20], Batch [40/168], Loss: 0.0758, Acc: 93.44%\n",
      "Epoch [6/20], Batch [50/168], Loss: 0.1525, Acc: 93.00%\n",
      "Epoch [6/20], Batch [60/168], Loss: 0.2468, Acc: 92.71%\n",
      "Epoch [6/20], Batch [70/168], Loss: 0.0731, Acc: 92.95%\n",
      "Epoch [6/20], Batch [80/168], Loss: 0.1084, Acc: 93.36%\n",
      "Epoch [6/20], Batch [90/168], Loss: 0.1432, Acc: 93.61%\n",
      "Epoch [6/20], Batch [100/168], Loss: 0.0255, Acc: 93.62%\n",
      "Epoch [6/20], Batch [110/168], Loss: 0.2404, Acc: 93.18%\n",
      "Epoch [6/20], Batch [120/168], Loss: 0.0651, Acc: 93.23%\n",
      "Epoch [6/20], Batch [130/168], Loss: 0.1306, Acc: 93.37%\n",
      "Epoch [6/20], Batch [140/168], Loss: 0.0346, Acc: 93.30%\n",
      "Epoch [6/20], Batch [150/168], Loss: 0.0590, Acc: 93.46%\n",
      "Epoch [6/20], Batch [160/168], Loss: 0.2783, Acc: 93.67%\n",
      "Epoch [6/20], Train Loss: 0.1497, Train Acc: 93.60%, Val Loss: 0.2842, Val Acc: 89.35%, Time: 56.29s\n",
      "Epoch [7/20], Batch [10/168], Loss: 0.1106, Acc: 92.50%\n",
      "Epoch [7/20], Batch [20/168], Loss: 0.1708, Acc: 93.12%\n",
      "Epoch [7/20], Batch [30/168], Loss: 0.1888, Acc: 94.58%\n",
      "Epoch [7/20], Batch [40/168], Loss: 0.0164, Acc: 94.06%\n",
      "Epoch [7/20], Batch [50/168], Loss: 0.1752, Acc: 93.88%\n",
      "Epoch [7/20], Batch [60/168], Loss: 0.1826, Acc: 93.65%\n",
      "Epoch [7/20], Batch [70/168], Loss: 0.1129, Acc: 94.11%\n",
      "Epoch [7/20], Batch [80/168], Loss: 0.1292, Acc: 94.06%\n",
      "Epoch [7/20], Batch [90/168], Loss: 0.2531, Acc: 94.17%\n",
      "Epoch [7/20], Batch [100/168], Loss: 0.2680, Acc: 94.31%\n",
      "Epoch [7/20], Batch [110/168], Loss: 0.0609, Acc: 94.38%\n",
      "Epoch [7/20], Batch [120/168], Loss: 0.0704, Acc: 94.27%\n",
      "Epoch [7/20], Batch [130/168], Loss: 0.1031, Acc: 94.38%\n",
      "Epoch [7/20], Batch [140/168], Loss: 0.1446, Acc: 94.33%\n",
      "Epoch [7/20], Batch [150/168], Loss: 0.6191, Acc: 94.17%\n",
      "Epoch [7/20], Batch [160/168], Loss: 0.0023, Acc: 94.22%\n",
      "Epoch [7/20], Train Loss: 0.1349, Train Acc: 94.38%, Val Loss: 0.1378, Val Acc: 95.32%, Time: 56.36s\n",
      "Epoch [8/20], Batch [10/168], Loss: 0.0375, Acc: 96.88%\n",
      "Epoch [8/20], Batch [20/168], Loss: 0.2413, Acc: 95.94%\n",
      "Epoch [8/20], Batch [30/168], Loss: 0.0586, Acc: 95.83%\n",
      "Epoch [8/20], Batch [40/168], Loss: 0.0023, Acc: 95.16%\n",
      "Epoch [8/20], Batch [50/168], Loss: 0.1717, Acc: 95.38%\n",
      "Epoch [8/20], Batch [60/168], Loss: 0.2615, Acc: 94.58%\n",
      "Epoch [8/20], Batch [70/168], Loss: 0.0859, Acc: 93.84%\n",
      "Epoch [8/20], Batch [80/168], Loss: 0.0272, Acc: 93.98%\n",
      "Epoch [8/20], Batch [90/168], Loss: 0.0337, Acc: 93.54%\n",
      "Epoch [8/20], Batch [100/168], Loss: 0.3598, Acc: 93.50%\n",
      "Epoch [8/20], Batch [110/168], Loss: 0.2118, Acc: 93.58%\n",
      "Epoch [8/20], Batch [120/168], Loss: 0.0322, Acc: 93.54%\n",
      "Epoch [8/20], Batch [130/168], Loss: 0.1642, Acc: 93.51%\n",
      "Epoch [8/20], Batch [140/168], Loss: 0.1162, Acc: 93.71%\n",
      "Epoch [8/20], Batch [150/168], Loss: 0.0237, Acc: 93.96%\n",
      "Epoch [8/20], Batch [160/168], Loss: 0.0484, Acc: 93.83%\n",
      "Epoch [8/20], Train Loss: 0.1467, Train Acc: 93.64%, Val Loss: 0.1516, Val Acc: 95.84%, Time: 56.38s\n",
      "Epoch [9/20], Batch [10/168], Loss: 0.1276, Acc: 95.62%\n",
      "Epoch [9/20], Batch [20/168], Loss: 0.0220, Acc: 97.50%\n",
      "Epoch [9/20], Batch [30/168], Loss: 0.1492, Acc: 96.88%\n",
      "Epoch [9/20], Batch [40/168], Loss: 0.0215, Acc: 95.94%\n",
      "Epoch [9/20], Batch [50/168], Loss: 0.2126, Acc: 95.88%\n",
      "Epoch [9/20], Batch [60/168], Loss: 0.2018, Acc: 96.25%\n",
      "Epoch [9/20], Batch [70/168], Loss: 0.0196, Acc: 96.25%\n",
      "Epoch [9/20], Batch [80/168], Loss: 0.0697, Acc: 96.09%\n",
      "Epoch [9/20], Batch [90/168], Loss: 0.1776, Acc: 95.49%\n",
      "Epoch [9/20], Batch [100/168], Loss: 0.0243, Acc: 95.50%\n",
      "Epoch [9/20], Batch [110/168], Loss: 0.2489, Acc: 95.57%\n",
      "Epoch [9/20], Batch [120/168], Loss: 0.1363, Acc: 95.52%\n",
      "Epoch [9/20], Batch [130/168], Loss: 0.0185, Acc: 95.77%\n",
      "Epoch [9/20], Batch [140/168], Loss: 0.3645, Acc: 95.85%\n",
      "Epoch [9/20], Batch [150/168], Loss: 0.1915, Acc: 95.58%\n",
      "Epoch [9/20], Batch [160/168], Loss: 0.0907, Acc: 95.70%\n",
      "Epoch [9/20], Train Loss: 0.1038, Train Acc: 95.68%, Val Loss: 0.1701, Val Acc: 95.32%, Time: 56.39s\n",
      "Epoch [10/20], Batch [10/168], Loss: 0.0319, Acc: 96.88%\n",
      "Epoch [10/20], Batch [20/168], Loss: 0.0161, Acc: 96.25%\n",
      "Epoch [10/20], Batch [30/168], Loss: 0.2237, Acc: 96.25%\n",
      "Epoch [10/20], Batch [40/168], Loss: 0.1104, Acc: 96.09%\n",
      "Epoch [10/20], Batch [50/168], Loss: 0.1941, Acc: 95.88%\n",
      "Epoch [10/20], Batch [60/168], Loss: 0.0383, Acc: 95.83%\n",
      "Epoch [10/20], Batch [70/168], Loss: 0.1125, Acc: 95.89%\n",
      "Epoch [10/20], Batch [80/168], Loss: 0.0294, Acc: 96.02%\n",
      "Epoch [10/20], Batch [90/168], Loss: 0.2374, Acc: 95.97%\n",
      "Epoch [10/20], Batch [100/168], Loss: 0.0290, Acc: 96.00%\n",
      "Epoch [10/20], Batch [110/168], Loss: 0.0444, Acc: 96.08%\n",
      "Epoch [10/20], Batch [120/168], Loss: 0.1076, Acc: 96.20%\n",
      "Epoch [10/20], Batch [130/168], Loss: 0.1212, Acc: 96.01%\n",
      "Epoch [10/20], Batch [140/168], Loss: 0.1752, Acc: 95.85%\n",
      "Epoch [10/20], Batch [150/168], Loss: 0.0454, Acc: 95.79%\n",
      "Epoch [10/20], Batch [160/168], Loss: 0.0372, Acc: 95.74%\n",
      "Epoch [10/20], Train Loss: 0.0960, Train Acc: 95.83%, Val Loss: 0.1710, Val Acc: 94.55%, Time: 56.46s\n",
      "Epoch [11/20], Batch [10/168], Loss: 0.1189, Acc: 93.75%\n",
      "Epoch [11/20], Batch [20/168], Loss: 0.0644, Acc: 95.31%\n",
      "Epoch [11/20], Batch [30/168], Loss: 0.2499, Acc: 94.79%\n",
      "Epoch [11/20], Batch [40/168], Loss: 0.0836, Acc: 94.84%\n",
      "Epoch [11/20], Batch [50/168], Loss: 0.0011, Acc: 95.38%\n",
      "Epoch [11/20], Batch [60/168], Loss: 0.0389, Acc: 95.10%\n",
      "Epoch [11/20], Batch [70/168], Loss: 0.2452, Acc: 95.18%\n",
      "Epoch [11/20], Batch [80/168], Loss: 0.0878, Acc: 95.39%\n",
      "Epoch [11/20], Batch [90/168], Loss: 0.0715, Acc: 95.35%\n",
      "Epoch [11/20], Batch [100/168], Loss: 0.0469, Acc: 95.56%\n",
      "Epoch [11/20], Batch [110/168], Loss: 0.0009, Acc: 95.80%\n",
      "Epoch [11/20], Batch [120/168], Loss: 0.0251, Acc: 95.83%\n",
      "Epoch [11/20], Batch [130/168], Loss: 0.0007, Acc: 96.01%\n",
      "Epoch [11/20], Batch [140/168], Loss: 0.0294, Acc: 96.07%\n",
      "Epoch [11/20], Batch [150/168], Loss: 0.1168, Acc: 95.96%\n",
      "Epoch [11/20], Batch [160/168], Loss: 0.1257, Acc: 95.98%\n",
      "Epoch [11/20], Train Loss: 0.0939, Train Acc: 95.94%, Val Loss: 0.1716, Val Acc: 94.81%, Time: 56.49s\n",
      "Epoch [12/20], Batch [10/168], Loss: 0.1782, Acc: 96.25%\n",
      "Epoch [12/20], Batch [20/168], Loss: 0.0873, Acc: 95.62%\n",
      "Epoch [12/20], Batch [30/168], Loss: 0.1319, Acc: 96.25%\n",
      "Epoch [12/20], Batch [40/168], Loss: 0.2032, Acc: 95.94%\n",
      "Epoch [12/20], Batch [50/168], Loss: 0.0829, Acc: 96.00%\n",
      "Epoch [12/20], Batch [60/168], Loss: 0.0346, Acc: 95.94%\n",
      "Epoch [12/20], Batch [70/168], Loss: 0.0571, Acc: 95.62%\n",
      "Epoch [12/20], Batch [80/168], Loss: 0.0813, Acc: 95.55%\n",
      "Epoch [12/20], Batch [90/168], Loss: 0.0823, Acc: 95.49%\n",
      "Epoch [12/20], Batch [100/168], Loss: 0.1259, Acc: 95.69%\n",
      "Epoch [12/20], Batch [110/168], Loss: 0.1311, Acc: 95.74%\n",
      "Epoch [12/20], Batch [120/168], Loss: 0.0282, Acc: 95.83%\n",
      "Epoch [12/20], Batch [130/168], Loss: 0.0270, Acc: 95.91%\n",
      "Epoch [12/20], Batch [140/168], Loss: 0.0289, Acc: 95.94%\n",
      "Epoch [12/20], Batch [150/168], Loss: 0.3205, Acc: 95.92%\n",
      "Epoch [12/20], Batch [160/168], Loss: 0.0823, Acc: 95.94%\n",
      "Epoch [12/20], Train Loss: 0.0927, Train Acc: 95.91%, Val Loss: 0.1758, Val Acc: 94.81%, Time: 56.53s\n",
      "Epoch [13/20], Batch [10/168], Loss: 0.1829, Acc: 97.50%\n",
      "Epoch [13/20], Batch [20/168], Loss: 0.0007, Acc: 97.19%\n",
      "Epoch [13/20], Batch [30/168], Loss: 0.0011, Acc: 97.50%\n",
      "Epoch [13/20], Batch [40/168], Loss: 0.1080, Acc: 96.88%\n",
      "Epoch [13/20], Batch [50/168], Loss: 0.1040, Acc: 96.50%\n",
      "Epoch [13/20], Batch [60/168], Loss: 0.0594, Acc: 96.46%\n",
      "Epoch [13/20], Batch [70/168], Loss: 0.1615, Acc: 96.61%\n",
      "Epoch [13/20], Batch [80/168], Loss: 0.0401, Acc: 96.25%\n",
      "Epoch [13/20], Batch [90/168], Loss: 0.0872, Acc: 96.04%\n",
      "Epoch [13/20], Batch [100/168], Loss: 0.0995, Acc: 96.25%\n",
      "Epoch [13/20], Batch [110/168], Loss: 0.1206, Acc: 96.14%\n",
      "Epoch [13/20], Batch [120/168], Loss: 0.0667, Acc: 96.30%\n",
      "Epoch [13/20], Batch [130/168], Loss: 0.2755, Acc: 96.35%\n",
      "Epoch [13/20], Batch [140/168], Loss: 0.1150, Acc: 96.34%\n",
      "Epoch [13/20], Batch [150/168], Loss: 0.1058, Acc: 96.17%\n",
      "Epoch [13/20], Batch [160/168], Loss: 0.0902, Acc: 96.02%\n",
      "Epoch [13/20], Train Loss: 0.0911, Train Acc: 95.94%, Val Loss: 0.1905, Val Acc: 94.55%, Time: 56.55s\n",
      "Epoch [14/20], Batch [10/168], Loss: 0.0744, Acc: 95.00%\n",
      "Epoch [14/20], Batch [20/168], Loss: 0.1707, Acc: 95.00%\n",
      "Epoch [14/20], Batch [30/168], Loss: 0.1018, Acc: 95.21%\n",
      "Epoch [14/20], Batch [40/168], Loss: 0.1237, Acc: 95.62%\n",
      "Epoch [14/20], Batch [50/168], Loss: 0.0245, Acc: 96.12%\n",
      "Epoch [14/20], Batch [60/168], Loss: 0.2714, Acc: 96.15%\n",
      "Epoch [14/20], Batch [70/168], Loss: 0.0784, Acc: 95.98%\n",
      "Epoch [14/20], Batch [80/168], Loss: 0.0666, Acc: 95.86%\n",
      "Epoch [14/20], Batch [90/168], Loss: 0.0436, Acc: 95.97%\n",
      "Epoch [14/20], Batch [100/168], Loss: 0.0992, Acc: 95.94%\n",
      "Epoch [14/20], Batch [110/168], Loss: 0.0004, Acc: 96.02%\n",
      "Epoch [14/20], Batch [120/168], Loss: 0.0405, Acc: 96.15%\n",
      "Epoch [14/20], Batch [130/168], Loss: 0.0718, Acc: 96.01%\n",
      "Epoch [14/20], Batch [140/168], Loss: 0.0006, Acc: 96.07%\n",
      "Epoch [14/20], Batch [150/168], Loss: 0.0404, Acc: 95.96%\n",
      "Epoch [14/20], Batch [160/168], Loss: 0.0284, Acc: 96.05%\n",
      "Epoch [14/20], Train Loss: 0.0904, Train Acc: 95.98%, Val Loss: 0.1870, Val Acc: 94.81%, Time: 56.58s\n",
      "Epoch [15/20], Batch [10/168], Loss: 0.0006, Acc: 98.12%\n",
      "Epoch [15/20], Batch [20/168], Loss: 0.1528, Acc: 97.19%\n",
      "Epoch [15/20], Batch [30/168], Loss: 0.0963, Acc: 96.88%\n",
      "Epoch [15/20], Batch [40/168], Loss: 0.0128, Acc: 97.19%\n",
      "Epoch [15/20], Batch [50/168], Loss: 0.0396, Acc: 96.25%\n",
      "Epoch [15/20], Batch [60/168], Loss: 0.1699, Acc: 96.35%\n",
      "Epoch [15/20], Batch [70/168], Loss: 0.0097, Acc: 96.34%\n",
      "Epoch [15/20], Batch [80/168], Loss: 0.1960, Acc: 96.25%\n",
      "Epoch [15/20], Batch [90/168], Loss: 0.0933, Acc: 96.11%\n",
      "Epoch [15/20], Batch [100/168], Loss: 0.0354, Acc: 96.12%\n",
      "Epoch [15/20], Batch [110/168], Loss: 0.0234, Acc: 95.97%\n",
      "Epoch [15/20], Batch [120/168], Loss: 0.0378, Acc: 96.20%\n",
      "Epoch [15/20], Batch [130/168], Loss: 0.0005, Acc: 96.25%\n",
      "Epoch [15/20], Batch [140/168], Loss: 0.0078, Acc: 96.25%\n",
      "Epoch [15/20], Batch [150/168], Loss: 0.0785, Acc: 96.21%\n",
      "Epoch [15/20], Batch [160/168], Loss: 0.1578, Acc: 96.02%\n",
      "Epoch [15/20], Train Loss: 0.0896, Train Acc: 95.94%, Val Loss: 0.1935, Val Acc: 95.32%, Time: 56.59s\n",
      "Epoch [16/20], Batch [10/168], Loss: 0.0314, Acc: 96.88%\n",
      "Epoch [16/20], Batch [20/168], Loss: 0.1953, Acc: 95.94%\n",
      "Epoch [16/20], Batch [30/168], Loss: 0.0376, Acc: 95.42%\n",
      "Epoch [16/20], Batch [40/168], Loss: 0.1432, Acc: 95.47%\n",
      "Epoch [16/20], Batch [50/168], Loss: 0.0193, Acc: 96.00%\n",
      "Epoch [16/20], Batch [60/168], Loss: 0.0187, Acc: 96.15%\n",
      "Epoch [16/20], Batch [70/168], Loss: 0.0293, Acc: 95.98%\n",
      "Epoch [16/20], Batch [80/168], Loss: 0.0520, Acc: 95.86%\n",
      "Epoch [16/20], Batch [90/168], Loss: 0.0954, Acc: 95.90%\n",
      "Epoch [16/20], Batch [100/168], Loss: 0.1123, Acc: 95.94%\n",
      "Epoch [16/20], Batch [110/168], Loss: 0.1107, Acc: 96.14%\n",
      "Epoch [16/20], Batch [120/168], Loss: 0.0333, Acc: 96.09%\n",
      "Epoch [16/20], Batch [130/168], Loss: 0.0483, Acc: 96.01%\n",
      "Epoch [16/20], Batch [140/168], Loss: 0.0935, Acc: 96.12%\n",
      "Epoch [16/20], Batch [150/168], Loss: 0.0903, Acc: 96.12%\n",
      "Epoch [16/20], Batch [160/168], Loss: 0.1350, Acc: 95.94%\n",
      "Epoch [16/20], Train Loss: 0.0887, Train Acc: 95.98%, Val Loss: 0.1959, Val Acc: 94.81%, Time: 56.60s\n",
      "Epoch [17/20], Batch [10/168], Loss: 0.0973, Acc: 96.88%\n",
      "Epoch [17/20], Batch [20/168], Loss: 0.3137, Acc: 95.62%\n",
      "Epoch [17/20], Batch [30/168], Loss: 0.0533, Acc: 96.25%\n",
      "Epoch [17/20], Batch [40/168], Loss: 0.0860, Acc: 96.88%\n",
      "Epoch [17/20], Batch [50/168], Loss: 0.1878, Acc: 96.25%\n",
      "Epoch [17/20], Batch [60/168], Loss: 0.0938, Acc: 96.15%\n",
      "Epoch [17/20], Batch [70/168], Loss: 0.1837, Acc: 95.98%\n",
      "Epoch [17/20], Batch [80/168], Loss: 0.0469, Acc: 96.02%\n",
      "Epoch [17/20], Batch [90/168], Loss: 0.1647, Acc: 95.90%\n",
      "Epoch [17/20], Batch [100/168], Loss: 0.0545, Acc: 95.75%\n",
      "Epoch [17/20], Batch [110/168], Loss: 0.0286, Acc: 95.91%\n",
      "Epoch [17/20], Batch [120/168], Loss: 0.1117, Acc: 95.83%\n",
      "Epoch [17/20], Batch [130/168], Loss: 0.1129, Acc: 95.87%\n",
      "Epoch [17/20], Batch [140/168], Loss: 0.0148, Acc: 95.89%\n",
      "Epoch [17/20], Batch [150/168], Loss: 0.0184, Acc: 95.96%\n",
      "Epoch [17/20], Batch [160/168], Loss: 0.0906, Acc: 95.86%\n",
      "Epoch [17/20], Train Loss: 0.0866, Train Acc: 95.94%, Val Loss: 0.2016, Val Acc: 94.81%, Time: 56.62s\n",
      "Epoch [18/20], Batch [10/168], Loss: 0.0966, Acc: 93.75%\n",
      "Epoch [18/20], Batch [20/168], Loss: 0.1534, Acc: 94.69%\n",
      "Epoch [18/20], Batch [30/168], Loss: 0.0477, Acc: 95.62%\n",
      "Epoch [18/20], Batch [40/168], Loss: 0.0462, Acc: 96.09%\n",
      "Epoch [18/20], Batch [50/168], Loss: 0.0152, Acc: 96.50%\n",
      "Epoch [18/20], Batch [60/168], Loss: 0.1318, Acc: 96.25%\n",
      "Epoch [18/20], Batch [70/168], Loss: 0.1880, Acc: 95.98%\n",
      "Epoch [18/20], Batch [80/168], Loss: 0.0845, Acc: 96.09%\n",
      "Epoch [18/20], Batch [90/168], Loss: 0.0728, Acc: 95.69%\n",
      "Epoch [18/20], Batch [100/168], Loss: 0.0404, Acc: 95.94%\n",
      "Epoch [18/20], Batch [110/168], Loss: 0.0813, Acc: 96.02%\n",
      "Epoch [18/20], Batch [120/168], Loss: 0.0726, Acc: 96.04%\n",
      "Epoch [18/20], Batch [130/168], Loss: 0.1155, Acc: 96.01%\n",
      "Epoch [18/20], Batch [140/168], Loss: 0.0808, Acc: 96.03%\n",
      "Epoch [18/20], Batch [150/168], Loss: 0.0804, Acc: 96.04%\n",
      "Epoch [18/20], Batch [160/168], Loss: 0.0008, Acc: 96.17%\n",
      "Epoch [18/20], Train Loss: 0.0857, Train Acc: 96.02%, Val Loss: 0.2043, Val Acc: 94.81%, Time: 56.65s\n",
      "Epoch [19/20], Batch [10/168], Loss: 0.0004, Acc: 98.75%\n",
      "Epoch [19/20], Batch [20/168], Loss: 0.0287, Acc: 97.50%\n",
      "Epoch [19/20], Batch [30/168], Loss: 0.0006, Acc: 97.71%\n",
      "Epoch [19/20], Batch [40/168], Loss: 0.2239, Acc: 97.03%\n",
      "Epoch [19/20], Batch [50/168], Loss: 0.0167, Acc: 97.12%\n",
      "Epoch [19/20], Batch [60/168], Loss: 0.1345, Acc: 96.98%\n",
      "Epoch [19/20], Batch [70/168], Loss: 0.2894, Acc: 96.34%\n",
      "Epoch [19/20], Batch [80/168], Loss: 0.0969, Acc: 96.56%\n",
      "Epoch [19/20], Batch [90/168], Loss: 0.1195, Acc: 96.32%\n",
      "Epoch [19/20], Batch [100/168], Loss: 0.2413, Acc: 96.25%\n",
      "Epoch [19/20], Batch [110/168], Loss: 0.0488, Acc: 96.42%\n",
      "Epoch [19/20], Batch [120/168], Loss: 0.1645, Acc: 96.15%\n",
      "Epoch [19/20], Batch [130/168], Loss: 0.1504, Acc: 95.96%\n",
      "Epoch [19/20], Batch [140/168], Loss: 0.0856, Acc: 95.94%\n",
      "Epoch [19/20], Batch [150/168], Loss: 0.0981, Acc: 95.88%\n",
      "Epoch [19/20], Batch [160/168], Loss: 0.0487, Acc: 95.94%\n",
      "Epoch [19/20], Train Loss: 0.0837, Train Acc: 95.98%, Val Loss: 0.2080, Val Acc: 94.81%, Time: 56.63s\n",
      "Epoch [20/20], Batch [10/168], Loss: 0.0798, Acc: 96.25%\n",
      "Epoch [20/20], Batch [20/168], Loss: 0.1126, Acc: 95.62%\n",
      "Epoch [20/20], Batch [30/168], Loss: 0.0005, Acc: 95.62%\n",
      "Epoch [20/20], Batch [40/168], Loss: 0.1816, Acc: 95.94%\n",
      "Epoch [20/20], Batch [50/168], Loss: 0.0013, Acc: 96.38%\n",
      "Epoch [20/20], Batch [60/168], Loss: 0.1199, Acc: 96.04%\n",
      "Epoch [20/20], Batch [70/168], Loss: 0.0624, Acc: 96.16%\n",
      "Epoch [20/20], Batch [80/168], Loss: 0.0530, Acc: 96.02%\n",
      "Epoch [20/20], Batch [90/168], Loss: 0.0755, Acc: 96.25%\n",
      "Epoch [20/20], Batch [100/168], Loss: 0.1514, Acc: 96.12%\n",
      "Epoch [20/20], Batch [110/168], Loss: 0.0193, Acc: 96.14%\n",
      "Epoch [20/20], Batch [120/168], Loss: 0.0450, Acc: 96.15%\n",
      "Epoch [20/20], Batch [130/168], Loss: 0.0006, Acc: 96.15%\n",
      "Epoch [20/20], Batch [140/168], Loss: 0.1204, Acc: 95.94%\n",
      "Epoch [20/20], Batch [150/168], Loss: 0.0398, Acc: 95.92%\n",
      "Epoch [20/20], Batch [160/168], Loss: 0.0838, Acc: 95.86%\n",
      "Epoch [20/20], Train Loss: 0.0827, Train Acc: 95.98%, Val Loss: 0.2158, Val Acc: 94.81%, Time: 56.66s\n",
      "\n",
      "Loading best model for evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_764290/1186937171.py:440: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 7 with validation accuracy 95.84%\n",
      "\n",
      "Evaluating best model...\n",
      "Test Accuracy: 92.71%\n",
      "Ambulance Left Accuracy: 98.33%\n",
      "Ambulance Right Accuracy: 98.82%\n",
      "Car Horn Left Accuracy: 97.00%\n",
      "Car Horn Right Accuracy: 100.00%\n",
      "Fire Truck Left Accuracy: 89.25%\n",
      "Fire Truck Right Accuracy: 57.47%\n",
      "Police Car Left Accuracy: 98.82%\n",
      "Police Car Right Accuracy: 97.94%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import time\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in monitored quantity to qualify as an improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "class DirectionalSoundDataset(Dataset):\n",
    "    def __init__(self, base_dir, transform=None, target_size=(224, 224)):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Define class mapping for all vehicle types and directions\n",
    "        self.class_to_idx = {\n",
    "            'ambulance_L': 0,\n",
    "            'ambulance_R': 1,\n",
    "            'carhorns_L': 2,\n",
    "            'carhorns_R': 3,\n",
    "            'FireTruck_L': 4,\n",
    "            'FireTruck_R': 5,\n",
    "            'policecar_L': 6,\n",
    "            'policecar_R': 7\n",
    "        }\n",
    "        \n",
    "        # Collect all files and their labels\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name in self.class_to_idx.keys():\n",
    "            class_dir = self.base_dir / class_name\n",
    "            if class_dir.exists():\n",
    "                class_files = list(class_dir.glob(f\"{class_name}_*.png\"))\n",
    "                self.files.extend(class_files)\n",
    "                self.labels.extend([self.class_to_idx[class_name]] * len(class_files))\n",
    "        \n",
    "        if len(self.files) == 0:\n",
    "            raise RuntimeError(f\"No spectrogram files found in {base_dir}\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load and process image\n",
    "        spectrogram = Image.open(img_path).convert('RGB')\n",
    "        if spectrogram.size != self.target_size:\n",
    "            spectrogram = spectrogram.resize(self.target_size)\n",
    "        \n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        return spectrogram, label\n",
    "\n",
    "def create_data_loaders(base_dir, batch_size=16):\n",
    "    \"\"\"\n",
    "    Create data loaders with 70-10-20 split for training, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): Path to the dataset directory\n",
    "        batch_size (int): Batch size for the data loaders\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Create full dataset\n",
    "    full_dataset = DirectionalSoundDataset(\n",
    "        base_dir=base_dir,\n",
    "        transform=transform,\n",
    "        target_size=(224, 224)\n",
    "    )\n",
    "    \n",
    "    # Calculate sizes\n",
    "    total_size = len(full_dataset)\n",
    "    test_size = 0.2  # 20% for test set\n",
    "    val_size = 0.1   # 10% for validation set\n",
    "    train_size = 0.7 # 70% for training set\n",
    "    \n",
    "    # Create indices for the splits\n",
    "    indices = list(range(total_size))\n",
    "    \n",
    "    # First split: separate out test set (20%)\n",
    "    train_val_idx, test_idx = train_test_split(\n",
    "        indices,\n",
    "        test_size=test_size,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Second split: separate training (70%) and validation (10%) from the remaining 80%\n",
    "    # To get 10% validation from the remaining 80%, we need val_size/(train_size + val_size) = 0.125\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        train_val_idx,\n",
    "        test_size=val_size/(train_size + val_size),  # This gives us 10% of total dataset\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Verify split sizes\n",
    "    print(f\"\\nDataset split sizes:\")\n",
    "    print(f\"Total dataset size: {total_size}\")\n",
    "    print(f\"Training set size: {len(train_idx)} ({len(train_idx)/total_size*100:.1f}%)\")\n",
    "    print(f\"Validation set size: {len(val_idx)} ({len(val_idx)/total_size*100:.1f}%)\")\n",
    "    print(f\"Test set size: {len(test_idx)} ({len(test_idx)/total_size*100:.1f}%)\\n\")\n",
    "    \n",
    "    # Create samplers\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        full_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        full_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=val_sampler,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        full_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=test_sampler,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Print dataset information\n",
    "    print(\"Dataset class distribution:\")\n",
    "    class_counts = {class_name: 0 for class_name in full_dataset.class_to_idx.keys()}\n",
    "    for idx in range(len(full_dataset)):\n",
    "        _, label = full_dataset[idx]\n",
    "        for class_name, class_idx in full_dataset.class_to_idx.items():\n",
    "            if label == class_idx:\n",
    "                class_counts[class_name] += 1\n",
    "    \n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"{class_name}: {count} samples\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "class DirectionalSoundViT(nn.Module):\n",
    "    def __init__(self, num_classes=8):  # Updated to 12 classes\n",
    "        super().__init__()\n",
    "        self.vit = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        num_features = self.vit.heads.head.in_features\n",
    "        self.vit.heads.head = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vit(x)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_metrics(train_losses, val_losses, train_accs, val_accs, save_path='training_metrics.png'):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics.\n",
    "    \n",
    "    Args:\n",
    "        train_losses (list): Training losses per epoch\n",
    "        val_losses (list): Validation losses per epoch\n",
    "        train_accs (list): Training accuracies per epoch\n",
    "        val_accs (list): Validation accuracies per epoch\n",
    "        save_path (str): Path to save the plot\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot accuracies\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accs, 'b-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accs, 'r-', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=30):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create directory for saving checkpoints\n",
    "    save_dir = Path('model_checkpoints')\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
    "                try:\n",
    "                    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                    \n",
    "                    outputs = model(spectrograms)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    \n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "                    if (batch_idx + 1) % 10 == 0:\n",
    "                        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                              f'Batch [{batch_idx+1}/{len(train_loader)}], '\n",
    "                              f'Loss: {loss.item():.4f}, '\n",
    "                              f'Acc: {100.*correct/total:.2f}%')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            train_acc = 100.*correct/total\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            epoch_time = time.time() - start_time\n",
    "            \n",
    "            # Store training metrics\n",
    "            train_losses.append(avg_train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for spectrograms, labels in val_loader:\n",
    "                    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                    outputs = model(spectrograms)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            val_acc = 100.*correct/total\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            # Store validation metrics\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "                  f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%, '\n",
    "                  f'Time: {epoch_time:.2f}s')\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                }, save_dir / 'short_best_model.pth')\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            # Plot and save training metrics after each epoch\n",
    "            plot_training_metrics(train_losses, val_losses, train_accs, val_accs)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {str(e)}\")\n",
    "        raise e\n",
    "    \n",
    "    # Return the training history\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for spectrograms, labels in test_loader:\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            outputs = model(spectrograms)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    # Print per-class accuracy\n",
    "    class_names = [\n",
    "        'Ambulance Left', 'Ambulance Right',\n",
    "        'Car Horn Left', 'Car Horn Right',\n",
    "        'Fire Truck Left',  'Fire Truck Right',\n",
    "        'Police Car Left',  'Police Car Right'\n",
    "    ]\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_mask = (all_labels == i)\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_acc = 100 * np.sum((all_predictions == i) & class_mask) / np.sum(class_mask)\n",
    "            print(f'{class_name} Accuracy: {class_acc:.2f}%')\n",
    "    \n",
    "    return accuracy, all_predictions, all_labels\n",
    "\n",
    "def save_model(model, optimizer, epoch, val_acc, filename):\n",
    "    \"\"\"Save model checkpoint with all necessary state information\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_acc': val_acc,\n",
    "    }, filename)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "def load_best_model(model, filepath):\n",
    "    \"\"\"Load the best model weights\"\"\"\n",
    "    if not Path(filepath).exists():\n",
    "        raise FileNotFoundError(f\"No model checkpoint found at {filepath}\")\n",
    "    \n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded model from epoch {checkpoint['epoch']} with validation accuracy {checkpoint['val_acc']:.2f}%\")\n",
    "    return model, checkpoint['epoch'], checkpoint['val_acc']\n",
    "\n",
    "def inference(model, image_path, device=None):\n",
    "    \"\"\"Run inference on a single image\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Define the same transforms used during training\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define class names\n",
    "    class_names = [\n",
    "        'Ambulance Left',  'Ambulance Right',\n",
    "        'Car Horn Left',  'Car Horn Right',\n",
    "        'Fire Truck Left',  'Fire Truck Right',\n",
    "        'Police Car Left', 'Police Car Right'\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "    return {\n",
    "        'predicted_class': class_names[predicted_class],\n",
    "        'confidence': confidence * 100,\n",
    "        'all_probabilities': {\n",
    "            class_name: prob.item() * 100 \n",
    "            for class_name, prob in zip(class_names, probabilities[0])\n",
    "        }\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the path to your dataset directory\n",
    "    base_dir = \"Dataset of warning sound types and source directions\"\n",
    "    checkpoint_dir = Path('model_checkpoints')\n",
    "    best_model_path = checkpoint_dir / 'short_best_model.pth'\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(base_dir, batch_size=16)\n",
    "    \n",
    "    # Create model with 12 classes\n",
    "    model = DirectionalSoundViT(num_classes=8)\n",
    "    \n",
    "    # Training phase\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_loader, val_loader, num_epochs=20)\n",
    "    \n",
    "    # Load the best model for evaluation\n",
    "    print(\"\\nLoading best model for evaluation...\")\n",
    "    model, best_epoch, best_val_acc = load_best_model(model, best_model_path)\n",
    "    \n",
    "    # Evaluate the best model\n",
    "    print(\"\\nEvaluating best model...\")\n",
    "    accuracy, predictions, labels = evaluate_model(model, test_loader)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading best model for evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_400375/1928624511.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 14 with validation accuracy 97.16%\n",
      "Loaded model from epoch 14 with validation accuracy 97.16%\n",
      "\n",
      "Running example inference...\n",
      "\n",
      "Inference results:\n",
      "Predicted class: Car Horn Right\n",
      "Confidence: 71.23%\n",
      "\n",
      "Inference Time:\n",
      "Average: 18.3043 milliseconds\n",
      "Standard Deviation: 13.8734 milliseconds\n",
      "\n",
      "All class probabilities:\n",
      "Ambulance Left: 0.37%\n",
      "Ambulance Middle: 0.90%\n",
      "Ambulance Right: 7.53%\n",
      "Car Horn Left: 0.07%\n",
      "Car Horn Middle: 0.02%\n",
      "Car Horn Right: 71.23%\n",
      "Fire Truck Left: 0.11%\n",
      "Fire Truck Middle: 0.09%\n",
      "Fire Truck Right: 0.16%\n",
      "Police Car Left: 0.38%\n",
      "Police Car Middle: 0.54%\n",
      "Police Car Right: 18.59%\n"
     ]
    }
   ],
   "source": [
    "def test_model_inference():\n",
    "    # Load the model\n",
    "    model = DirectionalSoundViT(num_classes=8)\n",
    "    print(\"\\nLoading best model for evaluation...\")\n",
    "    model, best_epoch, best_val_acc = load_best_model(model, 'model_checkpoints/short_best_model.pth')\n",
    "    print(f\"Loaded model from epoch {best_epoch} with validation accuracy {best_val_acc:.2f}%\")\n",
    "\n",
    "    # Run example inference\n",
    "    print(\"\\nRunning example inference...\")\n",
    "    test_image_path = \"./Dataset of warning sound types and source directions/noise/noise_16.png\"\n",
    "    # test_image_path = \"./test/test_output/final_stitched.png\"\n",
    "\n",
    "    \n",
    "    if Path(test_image_path).exists():\n",
    "        # Run multiple inference iterations\n",
    "        num_runs = 10\n",
    "        times = []\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.perf_counter()\n",
    "            result = inference(model, test_image_path)\n",
    "            end_time = time.perf_counter()\n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        import statistics\n",
    "        avg_time = statistics.mean(times) * 1000\n",
    "        std_dev = statistics.stdev(times) * 1000\n",
    "\n",
    "\n",
    "        print(f\"\\nInference results:\")\n",
    "        print(f\"Predicted class: {result['predicted_class']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.2f}%\")\n",
    "        print(f\"\\nInference Time:\")\n",
    "        print(f\"Average: {avg_time:.4f} milliseconds\")\n",
    "        print(f\"Standard Deviation: {std_dev:.4f} milliseconds\")\n",
    "        print(\"\\nAll class probabilities:\")\n",
    "        for class_name, prob in result['all_probabilities'].items():\n",
    "            print(f\"{class_name}: {prob:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Error: Test image not found at {test_image_path}\")\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_metrics(train_losses, val_losses, train_accs, val_accs, save_path='training_metrics.png'):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics.\n",
    "    \n",
    "    Args:\n",
    "        train_losses (list): Training losses per epoch\n",
    "        val_losses (list): Validation losses per epoch\n",
    "        train_accs (list): Training accuracies per epoch\n",
    "        val_accs (list): Validation accuracies per epoch\n",
    "        save_path (str): Path to save the plot\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot accuracies\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accs, 'b-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accs, 'r-', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=30):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create directory for saving checkpoints\n",
    "    save_dir = Path('model_checkpoints')\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
    "                try:\n",
    "                    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                    \n",
    "                    outputs = model(spectrograms)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    \n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "                    if (batch_idx + 1) % 10 == 0:\n",
    "                        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                              f'Batch [{batch_idx+1}/{len(train_loader)}], '\n",
    "                              f'Loss: {loss.item():.4f}, '\n",
    "                              f'Acc: {100.*correct/total:.2f}%')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            train_acc = 100.*correct/total\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            epoch_time = time.time() - start_time\n",
    "            \n",
    "            # Store training metrics\n",
    "            train_losses.append(avg_train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for spectrograms, labels in val_loader:\n",
    "                    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                    outputs = model(spectrograms)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            val_acc = 100.*correct/total\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            # Store validation metrics\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "                  f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%, '\n",
    "                  f'Time: {epoch_time:.2f}s')\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                }, save_dir / 'short_best_model.pth')\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            # Plot and save training metrics after each epoch\n",
    "            plot_training_metrics(train_losses, val_losses, train_accs, val_accs)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {str(e)}\")\n",
    "        raise e\n",
    "    \n",
    "    # Return the training history\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
