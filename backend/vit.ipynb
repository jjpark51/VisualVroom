{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Using device: cuda\n",
      "Epoch [1/30], Batch [10/116], Loss: 2.0621, Acc: 14.69%\n",
      "Epoch [1/30], Batch [20/116], Loss: 1.7429, Acc: 22.19%\n",
      "Epoch [1/30], Batch [30/116], Loss: 1.3671, Acc: 25.62%\n",
      "Epoch [1/30], Batch [40/116], Loss: 1.1493, Acc: 28.12%\n",
      "Epoch [1/30], Batch [50/116], Loss: 1.1407, Acc: 30.06%\n",
      "Epoch [1/30], Batch [60/116], Loss: 1.1808, Acc: 31.09%\n",
      "Epoch [1/30], Batch [70/116], Loss: 1.1746, Acc: 32.14%\n",
      "Epoch [1/30], Batch [80/116], Loss: 1.2092, Acc: 32.77%\n",
      "Epoch [1/30], Batch [90/116], Loss: 1.0133, Acc: 34.10%\n",
      "Epoch [1/30], Batch [100/116], Loss: 1.2345, Acc: 34.50%\n",
      "Epoch [1/30], Batch [110/116], Loss: 1.0898, Acc: 34.86%\n",
      "Epoch [1/30], Train Loss: 1.3790, Train Acc: 35.28%, Val Loss: 1.1333, Val Acc: 41.67%, Time: 71.76s\n",
      "Epoch [2/30], Batch [10/116], Loss: 1.0982, Acc: 43.12%\n",
      "Epoch [2/30], Batch [20/116], Loss: 0.9869, Acc: 45.94%\n",
      "Epoch [2/30], Batch [30/116], Loss: 1.3485, Acc: 46.56%\n",
      "Epoch [2/30], Batch [40/116], Loss: 0.9393, Acc: 47.50%\n",
      "Epoch [2/30], Batch [50/116], Loss: 1.0487, Acc: 47.88%\n",
      "Epoch [2/30], Batch [60/116], Loss: 0.8196, Acc: 49.95%\n",
      "Epoch [2/30], Batch [70/116], Loss: 0.8472, Acc: 51.52%\n",
      "Epoch [2/30], Batch [80/116], Loss: 1.0575, Acc: 52.03%\n",
      "Epoch [2/30], Batch [90/116], Loss: 0.9573, Acc: 52.64%\n",
      "Epoch [2/30], Batch [100/116], Loss: 0.7795, Acc: 53.69%\n",
      "Epoch [2/30], Batch [110/116], Loss: 0.5557, Acc: 55.17%\n",
      "Epoch [2/30], Train Loss: 0.9463, Train Acc: 55.60%, Val Loss: 0.7370, Val Acc: 68.56%, Time: 72.34s\n",
      "Epoch [3/30], Batch [10/116], Loss: 1.2443, Acc: 69.69%\n",
      "Epoch [3/30], Batch [20/116], Loss: 0.7640, Acc: 68.28%\n",
      "Epoch [3/30], Batch [30/116], Loss: 0.8371, Acc: 68.02%\n",
      "Epoch [3/30], Batch [40/116], Loss: 0.7052, Acc: 70.16%\n",
      "Epoch [3/30], Batch [50/116], Loss: 0.6065, Acc: 71.19%\n",
      "Epoch [3/30], Batch [60/116], Loss: 0.4421, Acc: 72.55%\n",
      "Epoch [3/30], Batch [70/116], Loss: 0.2286, Acc: 73.66%\n",
      "Epoch [3/30], Batch [80/116], Loss: 0.3611, Acc: 75.08%\n",
      "Epoch [3/30], Batch [90/116], Loss: 0.6197, Acc: 75.76%\n",
      "Epoch [3/30], Batch [100/116], Loss: 0.3569, Acc: 75.97%\n",
      "Epoch [3/30], Batch [110/116], Loss: 0.4229, Acc: 76.05%\n",
      "Epoch [3/30], Train Loss: 0.6007, Train Acc: 76.30%, Val Loss: 0.4173, Val Acc: 85.23%, Time: 72.63s\n",
      "Epoch [4/30], Batch [10/116], Loss: 0.2528, Acc: 89.06%\n",
      "Epoch [4/30], Batch [20/116], Loss: 0.2048, Acc: 90.16%\n",
      "Epoch [4/30], Batch [30/116], Loss: 0.2179, Acc: 90.83%\n",
      "Epoch [4/30], Batch [40/116], Loss: 0.1338, Acc: 91.64%\n",
      "Epoch [4/30], Batch [50/116], Loss: 0.3639, Acc: 91.19%\n",
      "Epoch [4/30], Batch [60/116], Loss: 0.4640, Acc: 91.35%\n",
      "Epoch [4/30], Batch [70/116], Loss: 0.1771, Acc: 91.25%\n",
      "Epoch [4/30], Batch [80/116], Loss: 0.2118, Acc: 91.41%\n",
      "Epoch [4/30], Batch [90/116], Loss: 0.2864, Acc: 91.18%\n",
      "Epoch [4/30], Batch [100/116], Loss: 0.1473, Acc: 91.16%\n",
      "Epoch [4/30], Batch [110/116], Loss: 0.1990, Acc: 91.14%\n",
      "Epoch [4/30], Train Loss: 0.2508, Train Acc: 91.23%, Val Loss: 0.2593, Val Acc: 91.29%, Time: 72.81s\n",
      "Epoch [5/30], Batch [10/116], Loss: 0.0368, Acc: 96.88%\n",
      "Epoch [5/30], Batch [20/116], Loss: 0.2439, Acc: 95.94%\n",
      "Epoch [5/30], Batch [30/116], Loss: 0.0278, Acc: 96.15%\n",
      "Epoch [5/30], Batch [40/116], Loss: 0.0182, Acc: 96.80%\n",
      "Epoch [5/30], Batch [50/116], Loss: 0.1618, Acc: 96.69%\n",
      "Epoch [5/30], Batch [60/116], Loss: 0.0243, Acc: 97.03%\n",
      "Epoch [5/30], Batch [70/116], Loss: 0.1004, Acc: 96.92%\n",
      "Epoch [5/30], Batch [80/116], Loss: 0.0520, Acc: 96.48%\n",
      "Epoch [5/30], Batch [90/116], Loss: 0.2881, Acc: 96.15%\n",
      "Epoch [5/30], Batch [100/116], Loss: 0.1929, Acc: 95.66%\n",
      "Epoch [5/30], Batch [110/116], Loss: 0.1569, Acc: 95.60%\n",
      "Epoch [5/30], Train Loss: 0.1344, Train Acc: 95.40%, Val Loss: 0.3193, Val Acc: 89.20%, Time: 72.95s\n",
      "Epoch [6/30], Batch [10/116], Loss: 0.0276, Acc: 94.06%\n",
      "Epoch [6/30], Batch [20/116], Loss: 0.0681, Acc: 95.78%\n",
      "Epoch [6/30], Batch [30/116], Loss: 0.2084, Acc: 96.04%\n",
      "Epoch [6/30], Batch [40/116], Loss: 0.0787, Acc: 96.09%\n",
      "Epoch [6/30], Batch [50/116], Loss: 0.0869, Acc: 95.75%\n",
      "Epoch [6/30], Batch [60/116], Loss: 0.1361, Acc: 96.25%\n",
      "Epoch [6/30], Batch [70/116], Loss: 0.0521, Acc: 96.70%\n",
      "Epoch [6/30], Batch [80/116], Loss: 0.0240, Acc: 96.95%\n",
      "Epoch [6/30], Batch [90/116], Loss: 0.0285, Acc: 97.05%\n",
      "Epoch [6/30], Batch [100/116], Loss: 0.0474, Acc: 97.16%\n",
      "Epoch [6/30], Batch [110/116], Loss: 0.0205, Acc: 97.05%\n",
      "Epoch [6/30], Train Loss: 0.0919, Train Acc: 96.97%, Val Loss: 0.1995, Val Acc: 92.99%, Time: 73.06s\n",
      "Epoch [7/30], Batch [10/116], Loss: 0.0433, Acc: 97.50%\n",
      "Epoch [7/30], Batch [20/116], Loss: 0.0151, Acc: 98.12%\n",
      "Epoch [7/30], Batch [30/116], Loss: 0.0264, Acc: 98.44%\n",
      "Epoch [7/30], Batch [40/116], Loss: 0.0575, Acc: 98.67%\n",
      "Epoch [7/30], Batch [50/116], Loss: 0.0104, Acc: 98.88%\n",
      "Epoch [7/30], Batch [60/116], Loss: 0.0045, Acc: 99.06%\n",
      "Epoch [7/30], Batch [70/116], Loss: 0.0067, Acc: 99.20%\n",
      "Epoch [7/30], Batch [80/116], Loss: 0.0140, Acc: 99.30%\n",
      "Epoch [7/30], Batch [90/116], Loss: 0.0081, Acc: 99.34%\n",
      "Epoch [7/30], Batch [100/116], Loss: 0.0049, Acc: 99.41%\n",
      "Epoch [7/30], Batch [110/116], Loss: 0.0044, Acc: 99.46%\n",
      "Epoch [7/30], Train Loss: 0.0250, Train Acc: 99.49%, Val Loss: 0.1445, Val Acc: 95.64%, Time: 73.10s\n",
      "Epoch [8/30], Batch [10/116], Loss: 0.0019, Acc: 100.00%\n",
      "Epoch [8/30], Batch [20/116], Loss: 0.0021, Acc: 100.00%\n",
      "Epoch [8/30], Batch [30/116], Loss: 0.0023, Acc: 100.00%\n",
      "Epoch [8/30], Batch [40/116], Loss: 0.0014, Acc: 100.00%\n",
      "Epoch [8/30], Batch [50/116], Loss: 0.0016, Acc: 100.00%\n",
      "Epoch [8/30], Batch [60/116], Loss: 0.0019, Acc: 100.00%\n",
      "Epoch [8/30], Batch [70/116], Loss: 0.0032, Acc: 100.00%\n",
      "Epoch [8/30], Batch [80/116], Loss: 0.0016, Acc: 100.00%\n",
      "Epoch [8/30], Batch [90/116], Loss: 0.0016, Acc: 100.00%\n",
      "Epoch [8/30], Batch [100/116], Loss: 0.0014, Acc: 100.00%\n",
      "Epoch [8/30], Batch [110/116], Loss: 0.0015, Acc: 100.00%\n",
      "Epoch [8/30], Train Loss: 0.0020, Train Acc: 100.00%, Val Loss: 0.1354, Val Acc: 96.78%, Time: 73.12s\n",
      "Epoch [9/30], Batch [10/116], Loss: 0.0013, Acc: 100.00%\n",
      "Epoch [9/30], Batch [20/116], Loss: 0.0015, Acc: 100.00%\n",
      "Epoch [9/30], Batch [30/116], Loss: 0.0015, Acc: 100.00%\n",
      "Epoch [9/30], Batch [40/116], Loss: 0.0012, Acc: 100.00%\n",
      "Epoch [9/30], Batch [50/116], Loss: 0.0012, Acc: 100.00%\n",
      "Epoch [9/30], Batch [60/116], Loss: 0.0012, Acc: 100.00%\n",
      "Epoch [9/30], Batch [70/116], Loss: 0.0014, Acc: 100.00%\n",
      "Epoch [9/30], Batch [80/116], Loss: 0.0011, Acc: 100.00%\n",
      "Epoch [9/30], Batch [90/116], Loss: 0.0014, Acc: 100.00%\n",
      "Epoch [9/30], Batch [100/116], Loss: 0.0014, Acc: 100.00%\n",
      "Epoch [9/30], Batch [110/116], Loss: 0.0014, Acc: 100.00%\n",
      "Epoch [9/30], Train Loss: 0.0013, Train Acc: 100.00%, Val Loss: 0.1247, Val Acc: 96.78%, Time: 73.15s\n",
      "Epoch [10/30], Batch [10/116], Loss: 0.0011, Acc: 100.00%\n",
      "Epoch [10/30], Batch [20/116], Loss: 0.0013, Acc: 100.00%\n",
      "Epoch [10/30], Batch [30/116], Loss: 0.0010, Acc: 100.00%\n",
      "Epoch [10/30], Batch [40/116], Loss: 0.0012, Acc: 100.00%\n",
      "Epoch [10/30], Batch [50/116], Loss: 0.0011, Acc: 100.00%\n",
      "Epoch [10/30], Batch [60/116], Loss: 0.0011, Acc: 100.00%\n",
      "Epoch [10/30], Batch [70/116], Loss: 0.0014, Acc: 100.00%\n",
      "Epoch [10/30], Batch [80/116], Loss: 0.0010, Acc: 100.00%\n",
      "Epoch [10/30], Batch [90/116], Loss: 0.0010, Acc: 100.00%\n",
      "Epoch [10/30], Batch [100/116], Loss: 0.0011, Acc: 100.00%\n",
      "Epoch [10/30], Batch [110/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [10/30], Train Loss: 0.0011, Train Acc: 100.00%, Val Loss: 0.1227, Val Acc: 96.78%, Time: 73.28s\n",
      "Epoch [11/30], Batch [10/116], Loss: 0.0011, Acc: 100.00%\n",
      "Epoch [11/30], Batch [20/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [11/30], Batch [30/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [11/30], Batch [40/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [11/30], Batch [50/116], Loss: 0.0010, Acc: 100.00%\n",
      "Epoch [11/30], Batch [60/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [11/30], Batch [70/116], Loss: 0.0011, Acc: 100.00%\n",
      "Epoch [11/30], Batch [80/116], Loss: 0.0010, Acc: 100.00%\n",
      "Epoch [11/30], Batch [90/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [11/30], Batch [100/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [11/30], Batch [110/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [11/30], Train Loss: 0.0009, Train Acc: 100.00%, Val Loss: 0.1176, Val Acc: 96.97%, Time: 73.27s\n",
      "Epoch [12/30], Batch [10/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [12/30], Batch [20/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [12/30], Batch [30/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [12/30], Batch [40/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [12/30], Batch [50/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [12/30], Batch [60/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [12/30], Batch [70/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [12/30], Batch [80/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [12/30], Batch [90/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [12/30], Batch [100/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [12/30], Batch [110/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [12/30], Train Loss: 0.0008, Train Acc: 100.00%, Val Loss: 0.1180, Val Acc: 96.97%, Time: 73.23s\n",
      "Epoch [13/30], Batch [10/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [13/30], Batch [20/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [13/30], Batch [30/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [13/30], Batch [40/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [13/30], Batch [50/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [13/30], Batch [60/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [13/30], Batch [70/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [13/30], Batch [80/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [13/30], Batch [90/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [13/30], Batch [100/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [13/30], Batch [110/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [13/30], Train Loss: 0.0007, Train Acc: 100.00%, Val Loss: 0.1334, Val Acc: 96.97%, Time: 73.30s\n",
      "Epoch [14/30], Batch [10/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [14/30], Batch [20/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [14/30], Batch [30/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [14/30], Batch [40/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [14/30], Batch [50/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [14/30], Batch [60/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [14/30], Batch [70/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [14/30], Batch [80/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [14/30], Batch [90/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [14/30], Batch [100/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [14/30], Batch [110/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [14/30], Train Loss: 0.0007, Train Acc: 100.00%, Val Loss: 0.1216, Val Acc: 96.97%, Time: 73.31s\n",
      "Epoch [15/30], Batch [10/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [15/30], Batch [20/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [15/30], Batch [30/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [15/30], Batch [40/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [15/30], Batch [50/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [15/30], Batch [60/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [15/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [15/30], Batch [80/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [15/30], Batch [90/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [15/30], Batch [100/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [15/30], Batch [110/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [15/30], Train Loss: 0.0006, Train Acc: 100.00%, Val Loss: 0.1264, Val Acc: 97.16%, Time: 73.32s\n",
      "Epoch [16/30], Batch [10/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [16/30], Batch [20/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [16/30], Batch [30/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [16/30], Batch [40/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [16/30], Batch [50/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [16/30], Batch [60/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [16/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [16/30], Batch [80/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [16/30], Batch [90/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [16/30], Batch [100/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [16/30], Batch [110/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [16/30], Train Loss: 0.0006, Train Acc: 100.00%, Val Loss: 0.1207, Val Acc: 97.16%, Time: 73.32s\n",
      "Epoch [17/30], Batch [10/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [17/30], Batch [20/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [17/30], Batch [30/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [17/30], Batch [40/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [17/30], Batch [50/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [17/30], Batch [60/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [17/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [17/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [17/30], Batch [90/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [17/30], Batch [100/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [17/30], Batch [110/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [17/30], Train Loss: 0.0006, Train Acc: 100.00%, Val Loss: 0.1284, Val Acc: 97.16%, Time: 73.36s\n",
      "Epoch [18/30], Batch [10/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [18/30], Batch [20/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [30/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [18/30], Batch [40/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [50/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [60/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [80/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [90/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [100/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [110/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Train Loss: 0.0005, Train Acc: 100.00%, Val Loss: 0.1217, Val Acc: 97.16%, Time: 73.36s\n",
      "Epoch [19/30], Batch [10/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [19/30], Batch [20/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [19/30], Batch [30/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [19/30], Batch [40/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [19/30], Batch [50/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [19/30], Batch [60/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [19/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [19/30], Batch [80/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [19/30], Batch [90/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [19/30], Batch [100/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [19/30], Batch [110/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [19/30], Train Loss: 0.0005, Train Acc: 100.00%, Val Loss: 0.1289, Val Acc: 97.16%, Time: 73.37s\n",
      "Epoch [20/30], Batch [10/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [20/30], Batch [20/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [20/30], Batch [30/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [20/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [20/30], Batch [50/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [20/30], Batch [60/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [20/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [20/30], Batch [80/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [20/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [20/30], Batch [100/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [20/30], Batch [110/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [20/30], Train Loss: 0.0005, Train Acc: 100.00%, Val Loss: 0.1225, Val Acc: 97.16%, Time: 73.38s\n",
      "Epoch [21/30], Batch [10/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [21/30], Batch [20/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [21/30], Batch [30/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [21/30], Batch [40/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [21/30], Batch [50/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [21/30], Batch [60/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [21/30], Batch [70/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [21/30], Batch [80/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [21/30], Batch [90/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [21/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [21/30], Batch [110/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [21/30], Train Loss: 0.0005, Train Acc: 100.00%, Val Loss: 0.1234, Val Acc: 97.16%, Time: 73.37s\n",
      "Epoch [22/30], Batch [10/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [22/30], Batch [20/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [22/30], Batch [30/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [22/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [22/30], Batch [50/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [22/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [22/30], Batch [70/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [22/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [22/30], Batch [90/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [22/30], Batch [100/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [22/30], Batch [110/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [22/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.1229, Val Acc: 97.16%, Time: 73.39s\n",
      "Epoch [23/30], Batch [10/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Batch [20/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [23/30], Batch [30/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [23/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Batch [50/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [23/30], Batch [60/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [23/30], Batch [70/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.1366, Val Acc: 97.16%, Time: 73.40s\n",
      "Epoch [24/30], Batch [10/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [24/30], Batch [20/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [24/30], Batch [30/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [24/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [24/30], Batch [50/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [24/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [24/30], Batch [70/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [24/30], Batch [80/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [24/30], Batch [90/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [24/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [24/30], Batch [110/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [24/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.1241, Val Acc: 97.16%, Time: 73.41s\n",
      "Epoch [25/30], Batch [10/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [20/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [30/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [40/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [25/30], Batch [50/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [70/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [100/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [25/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.1319, Val Acc: 97.16%, Time: 73.39s\n",
      "Epoch [26/30], Batch [10/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [26/30], Batch [20/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [30/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [26/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [50/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [70/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.1235, Val Acc: 97.16%, Time: 73.39s\n",
      "Epoch [27/30], Batch [10/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [20/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [27/30], Batch [30/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [50/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [70/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.1234, Val Acc: 97.16%, Time: 73.40s\n",
      "Epoch [28/30], Batch [10/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [28/30], Batch [20/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [28/30], Batch [30/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [28/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [28/30], Batch [50/116], Loss: 0.0003, Acc: 100.00%\n",
      "Epoch [28/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [28/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [28/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [28/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [28/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [28/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [28/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.1235, Val Acc: 97.16%, Time: 73.41s\n",
      "Epoch [29/30], Batch [10/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [20/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [30/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [50/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [29/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [90/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [29/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.1319, Val Acc: 97.16%, Time: 73.39s\n",
      "Epoch [30/30], Batch [10/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Batch [20/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Batch [30/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Batch [40/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [30/30], Batch [50/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [30/30], Batch [60/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [30/30], Batch [70/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.1235, Val Acc: 97.16%, Time: 73.40s\n",
      "\n",
      "Loading best model for evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_400375/1928624511.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 14 with validation accuracy 97.16%\n",
      "\n",
      "Evaluating best model...\n",
      "Test Accuracy: 97.63%\n",
      "Ambulance Left Accuracy: 99.06%\n",
      "Ambulance Middle Accuracy: 100.00%\n",
      "Ambulance Right Accuracy: 94.85%\n",
      "Car Horn Left Accuracy: 100.00%\n",
      "Car Horn Middle Accuracy: 97.75%\n",
      "Car Horn Right Accuracy: 100.00%\n",
      "Fire Truck Left Accuracy: 98.15%\n",
      "Fire Truck Middle Accuracy: 95.08%\n",
      "Fire Truck Right Accuracy: 96.55%\n",
      "Police Car Left Accuracy: 100.00%\n",
      "Police Car Middle Accuracy: 96.77%\n",
      "Police Car Right Accuracy: 92.08%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import time\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in monitored quantity to qualify as an improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "class DirectionalSoundDataset(Dataset):\n",
    "    def __init__(self, base_dir, transform=None, target_size=(224, 224)):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Define class mapping for all vehicle types and directions\n",
    "        self.class_to_idx = {\n",
    "            'ambulance_L': 0,\n",
    "            'ambulance_M': 1,\n",
    "            'ambulance_R': 2,\n",
    "            'carhorns_L': 3,\n",
    "            'carhorns_M': 4,\n",
    "            'carhorns_R': 5,\n",
    "            'FireTruck_L': 6,\n",
    "            'FireTruck_M': 7,\n",
    "            'FireTruck_R': 8,\n",
    "            'policecar_L': 9,\n",
    "            'policecar_M': 10,\n",
    "            'policecar_R': 11\n",
    "        }\n",
    "        \n",
    "        # Collect all files and their labels\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name in self.class_to_idx.keys():\n",
    "            class_dir = self.base_dir / class_name\n",
    "            if class_dir.exists():\n",
    "                class_files = list(class_dir.glob(f\"{class_name}_*.png\"))\n",
    "                self.files.extend(class_files)\n",
    "                self.labels.extend([self.class_to_idx[class_name]] * len(class_files))\n",
    "        \n",
    "        if len(self.files) == 0:\n",
    "            raise RuntimeError(f\"No spectrogram files found in {base_dir}\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load and process image\n",
    "        spectrogram = Image.open(img_path).convert('RGB')\n",
    "        if spectrogram.size != self.target_size:\n",
    "            spectrogram = spectrogram.resize(self.target_size)\n",
    "        \n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        return spectrogram, label\n",
    "\n",
    "def create_data_loaders(base_dir, batch_size=32, test_size=0.2, val_size=0.1):\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Create full dataset\n",
    "    full_dataset = DirectionalSoundDataset(\n",
    "        base_dir=base_dir,\n",
    "        transform=transform,\n",
    "        target_size=(224, 224)\n",
    "    )\n",
    "    \n",
    "    # Calculate sizes\n",
    "    total_size = len(full_dataset)\n",
    "    indices = list(range(total_size))\n",
    "    \n",
    "    # First split into train and test\n",
    "    train_idx, test_idx = train_test_split(indices, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Then split train into train and val\n",
    "    train_idx, val_idx = train_test_split(train_idx, test_size=val_size/(1-test_size), random_state=42)\n",
    "    \n",
    "    # Create samplers\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    # Create data loaders with reduced num_workers to prevent memory issues\n",
    "    train_loader = DataLoader(\n",
    "        full_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        full_dataset, batch_size=batch_size, sampler=val_sampler,\n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        full_dataset, batch_size=batch_size, sampler=test_sampler,\n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "class DirectionalSoundViT(nn.Module):\n",
    "    def __init__(self, num_classes=12):  # Updated to 12 classes\n",
    "        super().__init__()\n",
    "        self.vit = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        num_features = self.vit.heads.head.in_features\n",
    "        self.vit.heads.head = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vit(x)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=30):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create directory for saving checkpoints\n",
    "    save_dir = Path('model_checkpoints')\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
    "                try:\n",
    "                    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                    \n",
    "                    outputs = model(spectrograms)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    \n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "                    if (batch_idx + 1) % 10 == 0:\n",
    "                        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                              f'Batch [{batch_idx+1}/{len(train_loader)}], '\n",
    "                              f'Loss: {loss.item():.4f}, '\n",
    "                              f'Acc: {100.*correct/total:.2f}%')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            train_acc = 100.*correct/total\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            epoch_time = time.time() - start_time\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for spectrograms, labels in val_loader:\n",
    "                    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                    outputs = model(spectrograms)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            val_acc = 100.*correct/total\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, '\n",
    "                  f'Time: {epoch_time:.2f}s')\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                }, save_dir / 'best_model.pth')\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for spectrograms, labels in test_loader:\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            outputs = model(spectrograms)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    # Print per-class accuracy\n",
    "    class_names = [\n",
    "        'Ambulance Left', 'Ambulance Middle', 'Ambulance Right',\n",
    "        'Car Horn Left', 'Car Horn Middle', 'Car Horn Right',\n",
    "        'Fire Truck Left', 'Fire Truck Middle', 'Fire Truck Right',\n",
    "        'Police Car Left', 'Police Car Middle', 'Police Car Right'\n",
    "    ]\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_mask = (all_labels == i)\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_acc = 100 * np.sum((all_predictions == i) & class_mask) / np.sum(class_mask)\n",
    "            print(f'{class_name} Accuracy: {class_acc:.2f}%')\n",
    "    \n",
    "    return accuracy, all_predictions, all_labels\n",
    "\n",
    "def save_model(model, optimizer, epoch, val_acc, filename):\n",
    "    \"\"\"Save model checkpoint with all necessary state information\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_acc': val_acc,\n",
    "    }, filename)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "def load_best_model(model, filepath):\n",
    "    \"\"\"Load the best model weights\"\"\"\n",
    "    if not Path(filepath).exists():\n",
    "        raise FileNotFoundError(f\"No model checkpoint found at {filepath}\")\n",
    "    \n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded model from epoch {checkpoint['epoch']} with validation accuracy {checkpoint['val_acc']:.2f}%\")\n",
    "    return model, checkpoint['epoch'], checkpoint['val_acc']\n",
    "\n",
    "def inference(model, image_path, device=None):\n",
    "    \"\"\"Run inference on a single image\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Define the same transforms used during training\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define class names\n",
    "    class_names = [\n",
    "        'Ambulance Left', 'Ambulance Middle', 'Ambulance Right',\n",
    "        'Car Horn Left', 'Car Horn Middle', 'Car Horn Right',\n",
    "        'Fire Truck Left', 'Fire Truck Middle', 'Fire Truck Right',\n",
    "        'Police Car Left', 'Police Car Middle', 'Police Car Right'\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "    return {\n",
    "        'predicted_class': class_names[predicted_class],\n",
    "        'confidence': confidence * 100,\n",
    "        'all_probabilities': {\n",
    "            class_name: prob.item() * 100 \n",
    "            for class_name, prob in zip(class_names, probabilities[0])\n",
    "        }\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the path to your dataset directory\n",
    "    base_dir = \"Dataset of warning sound types and source directions\"\n",
    "    checkpoint_dir = Path('model_checkpoints')\n",
    "    best_model_path = checkpoint_dir / 'best_model.pth'\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(base_dir, batch_size=32)\n",
    "    \n",
    "    # Create model with 12 classes\n",
    "    model = DirectionalSoundViT(num_classes=12)\n",
    "    \n",
    "    # Training phase\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_loader, val_loader, num_epochs=30)\n",
    "    \n",
    "    # Load the best model for evaluation\n",
    "    print(\"\\nLoading best model for evaluation...\")\n",
    "    model, best_epoch, best_val_acc = load_best_model(model, best_model_path)\n",
    "    \n",
    "    # Evaluate the best model\n",
    "    print(\"\\nEvaluating best model...\")\n",
    "    accuracy, predictions, labels = evaluate_model(model, test_loader)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading best model for evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_400375/1928624511.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 14 with validation accuracy 97.16%\n",
      "Loaded model from epoch 14 with validation accuracy 97.16%\n",
      "\n",
      "Running example inference...\n",
      "\n",
      "Inference results:\n",
      "Predicted class: Car Horn Left\n",
      "Confidence: 39.18%\n",
      "\n",
      "All class probabilities:\n",
      "Ambulance Left: 1.63%\n",
      "Ambulance Middle: 5.02%\n",
      "Ambulance Right: 3.26%\n",
      "Car Horn Left: 39.18%\n",
      "Car Horn Middle: 5.32%\n",
      "Car Horn Right: 4.40%\n",
      "Fire Truck Left: 0.33%\n",
      "Fire Truck Middle: 38.82%\n",
      "Fire Truck Right: 0.84%\n",
      "Police Car Left: 0.05%\n",
      "Police Car Middle: 0.92%\n",
      "Police Car Right: 0.23%\n"
     ]
    }
   ],
   "source": [
    "def test_model_inference():\n",
    "    # Load the model\n",
    "    model = DirectionalSoundViT(num_classes=12)\n",
    "    print(\"\\nLoading best model for evaluation...\")\n",
    "    model, best_epoch, best_val_acc = load_best_model(model, 'model_checkpoints/best_model.pth')\n",
    "    print(f\"Loaded model from epoch {best_epoch} with validation accuracy {best_val_acc:.2f}%\")\n",
    "\n",
    "    # Run example inference\n",
    "    print(\"\\nRunning example inference...\")\n",
    "    # test_image_path = \"./Dataset of warning sound types and source directions/noise/noise_22.png\"\n",
    "    test_image_path = \"./test/test_output/final_stitched.png\"\n",
    "\n",
    "    \n",
    "    if Path(test_image_path).exists():\n",
    "        result = inference(model, test_image_path)\n",
    "        \n",
    "        print(f\"\\nInference results:\")\n",
    "        print(f\"Predicted class: {result['predicted_class']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.2f}%\")\n",
    "        print(\"\\nAll class probabilities:\")\n",
    "        for class_name, prob in result['all_probabilities'].items():\n",
    "            print(f\"{class_name}: {prob:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Error: Test image not found at {test_image_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
