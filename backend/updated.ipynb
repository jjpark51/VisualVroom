{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 5904\n",
      "Training set size: 4132 (70.0%)\n",
      "Validation set size: 591 (10.0%)\n",
      "Test set size: 1181 (20.0%)\n",
      "\n",
      "Class distribution in splits:\n",
      "Class                Train        Val       Test\n",
      "---------------------------------------------\n",
      "0                    67.9%       8.1%      24.0%\n",
      "1                    69.4%      11.2%      19.4%\n",
      "2                    68.5%       9.6%      21.9%\n",
      "3                    72.3%       8.5%      19.2%\n",
      "4                    72.7%       8.5%      18.8%\n",
      "5                    68.8%      11.5%      19.8%\n",
      "6                    73.3%       9.8%      16.9%\n",
      "7                    71.0%       9.8%      19.2%\n",
      "8                    70.4%      11.0%      18.5%\n",
      "9                    68.5%       9.2%      22.3%\n",
      "10                   66.7%      10.6%      22.7%\n",
      "11                   71.2%      11.0%      17.7%\n",
      "12                   66.7%      14.6%      18.8%\n",
      "13                   68.0%      10.0%      22.0%\n",
      "14                   65.2%      17.4%      17.4%\n",
      "Starting training...\n",
      "Using device: cuda\n",
      "Epoch [1/30], Batch [10/259], Loss: 2.3410, Acc: 15.62%\n",
      "Epoch [1/30], Batch [20/259], Loss: 1.6610, Acc: 20.62%\n",
      "Epoch [1/30], Batch [30/259], Loss: 1.3357, Acc: 22.92%\n",
      "Epoch [1/30], Batch [40/259], Loss: 1.5287, Acc: 24.69%\n",
      "Epoch [1/30], Batch [50/259], Loss: 1.3225, Acc: 26.00%\n",
      "Epoch [1/30], Batch [60/259], Loss: 1.3087, Acc: 27.81%\n",
      "Epoch [1/30], Batch [70/259], Loss: 1.1747, Acc: 28.48%\n",
      "Epoch [1/30], Batch [80/259], Loss: 1.2034, Acc: 29.38%\n",
      "Epoch [1/30], Batch [90/259], Loss: 1.2172, Acc: 30.35%\n",
      "Epoch [1/30], Batch [100/259], Loss: 1.3518, Acc: 31.44%\n",
      "Epoch [1/30], Batch [110/259], Loss: 1.1010, Acc: 32.27%\n",
      "Epoch [1/30], Batch [120/259], Loss: 1.2695, Acc: 32.60%\n",
      "Epoch [1/30], Batch [130/259], Loss: 1.1097, Acc: 32.69%\n",
      "Epoch [1/30], Batch [140/259], Loss: 1.3032, Acc: 33.04%\n",
      "Epoch [1/30], Batch [150/259], Loss: 1.0409, Acc: 33.38%\n",
      "Epoch [1/30], Batch [160/259], Loss: 1.2572, Acc: 33.59%\n",
      "Epoch [1/30], Batch [170/259], Loss: 1.4470, Acc: 33.90%\n",
      "Epoch [1/30], Batch [180/259], Loss: 1.1844, Acc: 34.10%\n",
      "Epoch [1/30], Batch [190/259], Loss: 1.0184, Acc: 34.57%\n",
      "Epoch [1/30], Batch [200/259], Loss: 1.0709, Acc: 34.91%\n",
      "Epoch [1/30], Batch [210/259], Loss: 1.1929, Acc: 35.33%\n",
      "Epoch [1/30], Batch [220/259], Loss: 1.1113, Acc: 35.26%\n",
      "Epoch [1/30], Batch [230/259], Loss: 1.1060, Acc: 35.27%\n",
      "Epoch [1/30], Batch [240/259], Loss: 1.0056, Acc: 35.49%\n",
      "Epoch [1/30], Batch [250/259], Loss: 1.1118, Acc: 35.73%\n",
      "Epoch [1/30], Train Loss: 1.3105, Train Acc: 35.77%, Val Loss: 1.1581, Val Acc: 40.61%, Time: 85.28s\n",
      "Epoch [2/30], Batch [10/259], Loss: 1.0778, Acc: 39.38%\n",
      "Epoch [2/30], Batch [20/259], Loss: 1.0726, Acc: 43.44%\n",
      "Epoch [2/30], Batch [30/259], Loss: 1.0621, Acc: 44.38%\n",
      "Epoch [2/30], Batch [40/259], Loss: 1.0173, Acc: 44.38%\n",
      "Epoch [2/30], Batch [50/259], Loss: 1.1113, Acc: 45.50%\n",
      "Epoch [2/30], Batch [60/259], Loss: 0.9027, Acc: 46.46%\n",
      "Epoch [2/30], Batch [70/259], Loss: 1.3563, Acc: 46.16%\n",
      "Epoch [2/30], Batch [80/259], Loss: 1.2006, Acc: 45.39%\n",
      "Epoch [2/30], Batch [90/259], Loss: 0.8679, Acc: 46.25%\n",
      "Epoch [2/30], Batch [100/259], Loss: 0.9314, Acc: 47.06%\n",
      "Epoch [2/30], Batch [110/259], Loss: 0.6050, Acc: 48.07%\n",
      "Epoch [2/30], Batch [120/259], Loss: 1.1786, Acc: 49.11%\n",
      "Epoch [2/30], Batch [130/259], Loss: 0.8200, Acc: 49.71%\n",
      "Epoch [2/30], Batch [140/259], Loss: 1.0948, Acc: 50.58%\n",
      "Epoch [2/30], Batch [150/259], Loss: 0.5988, Acc: 50.88%\n",
      "Epoch [2/30], Batch [160/259], Loss: 1.2393, Acc: 51.21%\n",
      "Epoch [2/30], Batch [170/259], Loss: 0.9103, Acc: 51.99%\n",
      "Epoch [2/30], Batch [180/259], Loss: 0.8548, Acc: 52.57%\n",
      "Epoch [2/30], Batch [190/259], Loss: 0.9389, Acc: 53.03%\n",
      "Epoch [2/30], Batch [200/259], Loss: 0.9657, Acc: 53.28%\n",
      "Epoch [2/30], Batch [210/259], Loss: 0.7654, Acc: 53.45%\n",
      "Epoch [2/30], Batch [220/259], Loss: 0.8644, Acc: 53.84%\n",
      "Epoch [2/30], Batch [230/259], Loss: 0.5372, Acc: 54.18%\n",
      "Epoch [2/30], Batch [240/259], Loss: 0.8805, Acc: 54.66%\n",
      "Epoch [2/30], Batch [250/259], Loss: 0.9435, Acc: 55.12%\n",
      "Epoch [2/30], Train Loss: 0.9169, Train Acc: 55.57%, Val Loss: 0.6678, Val Acc: 68.53%, Time: 86.25s\n",
      "Epoch [3/30], Batch [10/259], Loss: 0.6219, Acc: 71.88%\n",
      "Epoch [3/30], Batch [20/259], Loss: 0.4541, Acc: 72.50%\n",
      "Epoch [3/30], Batch [30/259], Loss: 0.5176, Acc: 73.12%\n",
      "Epoch [3/30], Batch [40/259], Loss: 0.4564, Acc: 72.97%\n",
      "Epoch [3/30], Batch [50/259], Loss: 0.4893, Acc: 73.50%\n",
      "Epoch [3/30], Batch [60/259], Loss: 0.7541, Acc: 72.81%\n",
      "Epoch [3/30], Batch [70/259], Loss: 0.4974, Acc: 72.77%\n",
      "Epoch [3/30], Batch [80/259], Loss: 0.9337, Acc: 71.56%\n",
      "Epoch [3/30], Batch [90/259], Loss: 0.7239, Acc: 71.32%\n",
      "Epoch [3/30], Batch [100/259], Loss: 0.6479, Acc: 71.69%\n",
      "Epoch [3/30], Batch [110/259], Loss: 0.5449, Acc: 71.08%\n",
      "Epoch [3/30], Batch [120/259], Loss: 0.5067, Acc: 71.46%\n",
      "Epoch [3/30], Batch [130/259], Loss: 0.3804, Acc: 72.12%\n",
      "Epoch [3/30], Batch [140/259], Loss: 0.4480, Acc: 72.32%\n",
      "Epoch [3/30], Batch [150/259], Loss: 0.5441, Acc: 72.83%\n",
      "Epoch [3/30], Batch [160/259], Loss: 0.6402, Acc: 73.24%\n",
      "Epoch [3/30], Batch [170/259], Loss: 0.5308, Acc: 73.38%\n",
      "Epoch [3/30], Batch [180/259], Loss: 0.5965, Acc: 73.82%\n",
      "Epoch [3/30], Batch [190/259], Loss: 0.3724, Acc: 74.44%\n",
      "Epoch [3/30], Batch [200/259], Loss: 0.6808, Acc: 74.66%\n",
      "Epoch [3/30], Batch [210/259], Loss: 0.4705, Acc: 74.94%\n",
      "Epoch [3/30], Batch [220/259], Loss: 0.4579, Acc: 75.28%\n",
      "Epoch [3/30], Batch [230/259], Loss: 0.5889, Acc: 75.52%\n",
      "Epoch [3/30], Batch [240/259], Loss: 0.3018, Acc: 75.91%\n",
      "Epoch [3/30], Batch [250/259], Loss: 0.2223, Acc: 76.15%\n",
      "Epoch [3/30], Train Loss: 0.5623, Train Acc: 76.31%, Val Loss: 0.3905, Val Acc: 81.73%, Time: 86.61s\n",
      "Epoch [4/30], Batch [10/259], Loss: 0.2255, Acc: 91.88%\n",
      "Epoch [4/30], Batch [20/259], Loss: 0.2263, Acc: 90.62%\n",
      "Epoch [4/30], Batch [30/259], Loss: 0.3377, Acc: 89.58%\n",
      "Epoch [4/30], Batch [40/259], Loss: 0.1772, Acc: 89.84%\n",
      "Epoch [4/30], Batch [50/259], Loss: 0.3224, Acc: 89.50%\n",
      "Epoch [4/30], Batch [60/259], Loss: 0.4614, Acc: 89.27%\n",
      "Epoch [4/30], Batch [70/259], Loss: 0.1108, Acc: 89.02%\n",
      "Epoch [4/30], Batch [80/259], Loss: 0.4896, Acc: 88.44%\n",
      "Epoch [4/30], Batch [90/259], Loss: 0.5988, Acc: 86.11%\n",
      "Epoch [4/30], Batch [100/259], Loss: 0.4086, Acc: 85.00%\n",
      "Epoch [4/30], Batch [110/259], Loss: 0.7936, Acc: 84.26%\n",
      "Epoch [4/30], Batch [120/259], Loss: 0.3494, Acc: 84.27%\n",
      "Epoch [4/30], Batch [130/259], Loss: 0.4437, Acc: 84.47%\n",
      "Epoch [4/30], Batch [140/259], Loss: 0.3568, Acc: 84.78%\n",
      "Epoch [4/30], Batch [150/259], Loss: 0.2377, Acc: 85.12%\n",
      "Epoch [4/30], Batch [160/259], Loss: 0.3255, Acc: 85.35%\n",
      "Epoch [4/30], Batch [170/259], Loss: 0.1662, Acc: 85.81%\n",
      "Epoch [4/30], Batch [180/259], Loss: 0.3115, Acc: 85.97%\n",
      "Epoch [4/30], Batch [190/259], Loss: 0.1062, Acc: 85.99%\n",
      "Epoch [4/30], Batch [200/259], Loss: 0.3577, Acc: 85.78%\n",
      "Epoch [4/30], Batch [210/259], Loss: 0.2714, Acc: 85.89%\n",
      "Epoch [4/30], Batch [220/259], Loss: 0.0830, Acc: 86.16%\n",
      "Epoch [4/30], Batch [230/259], Loss: 0.6155, Acc: 86.25%\n",
      "Epoch [4/30], Batch [240/259], Loss: 0.0646, Acc: 86.51%\n",
      "Epoch [4/30], Batch [250/259], Loss: 0.3530, Acc: 86.67%\n",
      "Epoch [4/30], Train Loss: 0.3451, Train Acc: 86.57%, Val Loss: 0.3518, Val Acc: 83.59%, Time: 86.69s\n",
      "Epoch [5/30], Batch [10/259], Loss: 0.0978, Acc: 91.88%\n",
      "Epoch [5/30], Batch [20/259], Loss: 0.2020, Acc: 92.50%\n",
      "Epoch [5/30], Batch [30/259], Loss: 0.2367, Acc: 91.88%\n",
      "Epoch [5/30], Batch [40/259], Loss: 0.1330, Acc: 92.34%\n",
      "Epoch [5/30], Batch [50/259], Loss: 0.1017, Acc: 92.88%\n",
      "Epoch [5/30], Batch [60/259], Loss: 0.6923, Acc: 92.40%\n",
      "Epoch [5/30], Batch [70/259], Loss: 0.0202, Acc: 92.41%\n",
      "Epoch [5/30], Batch [80/259], Loss: 0.3465, Acc: 92.03%\n",
      "Epoch [5/30], Batch [90/259], Loss: 0.0718, Acc: 91.94%\n",
      "Epoch [5/30], Batch [100/259], Loss: 0.4240, Acc: 91.81%\n",
      "Epoch [5/30], Batch [110/259], Loss: 0.0570, Acc: 91.99%\n",
      "Epoch [5/30], Batch [120/259], Loss: 0.4672, Acc: 91.82%\n",
      "Epoch [5/30], Batch [130/259], Loss: 0.2911, Acc: 91.49%\n",
      "Epoch [5/30], Batch [140/259], Loss: 0.2962, Acc: 91.34%\n",
      "Epoch [5/30], Batch [150/259], Loss: 0.5067, Acc: 91.04%\n",
      "Epoch [5/30], Batch [160/259], Loss: 0.0823, Acc: 91.25%\n",
      "Epoch [5/30], Batch [170/259], Loss: 0.1978, Acc: 91.36%\n",
      "Epoch [5/30], Batch [180/259], Loss: 0.3641, Acc: 91.42%\n",
      "Epoch [5/30], Batch [190/259], Loss: 0.2153, Acc: 91.51%\n",
      "Epoch [5/30], Batch [200/259], Loss: 0.1903, Acc: 91.62%\n",
      "Epoch [5/30], Batch [210/259], Loss: 0.0786, Acc: 91.64%\n",
      "Epoch [5/30], Batch [220/259], Loss: 0.0410, Acc: 91.65%\n",
      "Epoch [5/30], Batch [230/259], Loss: 0.2028, Acc: 91.77%\n",
      "Epoch [5/30], Batch [240/259], Loss: 0.1545, Acc: 91.80%\n",
      "Epoch [5/30], Batch [250/259], Loss: 0.4680, Acc: 91.78%\n",
      "Epoch [5/30], Train Loss: 0.2028, Train Acc: 91.77%, Val Loss: 0.2488, Val Acc: 90.19%, Time: 86.72s\n",
      "Epoch [6/30], Batch [10/259], Loss: 0.3165, Acc: 94.38%\n",
      "Epoch [6/30], Batch [20/259], Loss: 0.1803, Acc: 92.50%\n",
      "Epoch [6/30], Batch [30/259], Loss: 0.0243, Acc: 94.17%\n",
      "Epoch [6/30], Batch [40/259], Loss: 0.2810, Acc: 93.75%\n",
      "Epoch [6/30], Batch [50/259], Loss: 0.1140, Acc: 93.62%\n",
      "Epoch [6/30], Batch [60/259], Loss: 0.0986, Acc: 93.54%\n",
      "Epoch [6/30], Batch [70/259], Loss: 0.1328, Acc: 93.57%\n",
      "Epoch [6/30], Batch [80/259], Loss: 0.0995, Acc: 93.67%\n",
      "Epoch [6/30], Batch [90/259], Loss: 0.5570, Acc: 93.47%\n",
      "Epoch [6/30], Batch [100/259], Loss: 0.0610, Acc: 93.25%\n",
      "Epoch [6/30], Batch [110/259], Loss: 0.4227, Acc: 93.12%\n",
      "Epoch [6/30], Batch [120/259], Loss: 0.1951, Acc: 93.18%\n",
      "Epoch [6/30], Batch [130/259], Loss: 0.1693, Acc: 93.27%\n",
      "Epoch [6/30], Batch [140/259], Loss: 0.1767, Acc: 93.21%\n",
      "Epoch [6/30], Batch [150/259], Loss: 0.0346, Acc: 93.25%\n",
      "Epoch [6/30], Batch [160/259], Loss: 0.1989, Acc: 93.09%\n",
      "Epoch [6/30], Batch [170/259], Loss: 0.1397, Acc: 93.20%\n",
      "Epoch [6/30], Batch [180/259], Loss: 0.2722, Acc: 93.30%\n",
      "Epoch [6/30], Batch [190/259], Loss: 0.2579, Acc: 93.32%\n",
      "Epoch [6/30], Batch [200/259], Loss: 0.1495, Acc: 93.16%\n",
      "Epoch [6/30], Batch [210/259], Loss: 0.1938, Acc: 93.10%\n",
      "Epoch [6/30], Batch [220/259], Loss: 0.2905, Acc: 93.01%\n",
      "Epoch [6/30], Batch [230/259], Loss: 0.5104, Acc: 92.96%\n",
      "Epoch [6/30], Batch [240/259], Loss: 0.0188, Acc: 93.07%\n",
      "Epoch [6/30], Batch [250/259], Loss: 0.2481, Acc: 93.10%\n",
      "Epoch [6/30], Train Loss: 0.1760, Train Acc: 93.10%, Val Loss: 0.3762, Val Acc: 86.13%, Time: 86.77s\n",
      "Epoch [7/30], Batch [10/259], Loss: 0.1160, Acc: 88.75%\n",
      "Epoch [7/30], Batch [20/259], Loss: 0.3443, Acc: 89.38%\n",
      "Epoch [7/30], Batch [30/259], Loss: 0.0470, Acc: 90.83%\n",
      "Epoch [7/30], Batch [40/259], Loss: 0.1910, Acc: 92.19%\n",
      "Epoch [7/30], Batch [50/259], Loss: 0.0882, Acc: 92.25%\n",
      "Epoch [7/30], Batch [60/259], Loss: 0.1792, Acc: 92.50%\n",
      "Epoch [7/30], Batch [70/259], Loss: 0.1351, Acc: 92.77%\n",
      "Epoch [7/30], Batch [80/259], Loss: 0.0723, Acc: 93.44%\n",
      "Epoch [7/30], Batch [90/259], Loss: 0.0790, Acc: 93.82%\n",
      "Epoch [7/30], Batch [100/259], Loss: 0.0248, Acc: 94.25%\n",
      "Epoch [7/30], Batch [110/259], Loss: 0.0187, Acc: 94.09%\n",
      "Epoch [7/30], Batch [120/259], Loss: 0.0342, Acc: 94.38%\n",
      "Epoch [7/30], Batch [130/259], Loss: 0.0037, Acc: 94.62%\n",
      "Epoch [7/30], Batch [140/259], Loss: 0.0322, Acc: 94.33%\n",
      "Epoch [7/30], Batch [150/259], Loss: 0.2803, Acc: 94.25%\n",
      "Epoch [7/30], Batch [160/259], Loss: 0.0507, Acc: 94.22%\n",
      "Epoch [7/30], Batch [170/259], Loss: 0.0551, Acc: 94.23%\n",
      "Epoch [7/30], Batch [180/259], Loss: 0.0363, Acc: 94.27%\n",
      "Epoch [7/30], Batch [190/259], Loss: 0.0975, Acc: 94.21%\n",
      "Epoch [7/30], Batch [200/259], Loss: 0.2723, Acc: 94.09%\n",
      "Epoch [7/30], Batch [210/259], Loss: 0.1047, Acc: 94.14%\n",
      "Epoch [7/30], Batch [220/259], Loss: 0.1858, Acc: 94.06%\n",
      "Epoch [7/30], Batch [230/259], Loss: 1.0839, Acc: 93.99%\n",
      "Epoch [7/30], Batch [240/259], Loss: 0.3124, Acc: 93.65%\n",
      "Epoch [7/30], Batch [250/259], Loss: 0.2703, Acc: 93.30%\n",
      "Epoch [7/30], Train Loss: 0.1760, Train Acc: 93.34%, Val Loss: 0.3367, Val Acc: 88.83%, Time: 86.82s\n",
      "Epoch [8/30], Batch [10/259], Loss: 0.0747, Acc: 97.50%\n",
      "Epoch [8/30], Batch [20/259], Loss: 0.1878, Acc: 94.69%\n",
      "Epoch [8/30], Batch [30/259], Loss: 0.0349, Acc: 95.00%\n",
      "Epoch [8/30], Batch [40/259], Loss: 0.0615, Acc: 95.16%\n",
      "Epoch [8/30], Batch [50/259], Loss: 0.0050, Acc: 95.38%\n",
      "Epoch [8/30], Batch [60/259], Loss: 0.1422, Acc: 95.42%\n",
      "Epoch [8/30], Batch [70/259], Loss: 0.3152, Acc: 95.36%\n",
      "Epoch [8/30], Batch [80/259], Loss: 0.1590, Acc: 95.00%\n",
      "Epoch [8/30], Batch [90/259], Loss: 0.0308, Acc: 95.28%\n",
      "Epoch [8/30], Batch [100/259], Loss: 0.1281, Acc: 95.44%\n",
      "Epoch [8/30], Batch [110/259], Loss: 0.0056, Acc: 95.11%\n",
      "Epoch [8/30], Batch [120/259], Loss: 0.0152, Acc: 95.21%\n",
      "Epoch [8/30], Batch [130/259], Loss: 0.3057, Acc: 95.00%\n",
      "Epoch [8/30], Batch [140/259], Loss: 0.1060, Acc: 94.91%\n",
      "Epoch [8/30], Batch [150/259], Loss: 0.6463, Acc: 94.75%\n",
      "Epoch [8/30], Batch [160/259], Loss: 0.0657, Acc: 95.00%\n",
      "Epoch [8/30], Batch [170/259], Loss: 0.2955, Acc: 95.04%\n",
      "Epoch [8/30], Batch [180/259], Loss: 0.0577, Acc: 95.03%\n",
      "Epoch [8/30], Batch [190/259], Loss: 0.1054, Acc: 94.93%\n",
      "Epoch [8/30], Batch [200/259], Loss: 0.1200, Acc: 94.88%\n",
      "Epoch [8/30], Batch [210/259], Loss: 0.1520, Acc: 95.00%\n",
      "Epoch [8/30], Batch [220/259], Loss: 0.0241, Acc: 95.06%\n",
      "Epoch [8/30], Batch [230/259], Loss: 0.0397, Acc: 95.08%\n",
      "Epoch [8/30], Batch [240/259], Loss: 0.0537, Acc: 95.23%\n",
      "Epoch [8/30], Batch [250/259], Loss: 0.0170, Acc: 95.10%\n",
      "Epoch [8/30], Train Loss: 0.1298, Train Acc: 95.04%, Val Loss: 0.1756, Val Acc: 92.55%, Time: 86.95s\n",
      "Epoch [9/30], Batch [10/259], Loss: 0.0078, Acc: 96.25%\n",
      "Epoch [9/30], Batch [20/259], Loss: 0.1475, Acc: 97.19%\n",
      "Epoch [9/30], Batch [30/259], Loss: 0.0817, Acc: 97.50%\n",
      "Epoch [9/30], Batch [40/259], Loss: 0.0086, Acc: 97.81%\n",
      "Epoch [9/30], Batch [50/259], Loss: 0.1687, Acc: 97.88%\n",
      "Epoch [9/30], Batch [60/259], Loss: 0.0259, Acc: 97.29%\n",
      "Epoch [9/30], Batch [70/259], Loss: 0.0191, Acc: 97.23%\n",
      "Epoch [9/30], Batch [80/259], Loss: 0.1372, Acc: 96.95%\n",
      "Epoch [9/30], Batch [90/259], Loss: 0.0227, Acc: 96.94%\n",
      "Epoch [9/30], Batch [100/259], Loss: 0.0431, Acc: 96.88%\n",
      "Epoch [9/30], Batch [110/259], Loss: 0.0974, Acc: 96.70%\n",
      "Epoch [9/30], Batch [120/259], Loss: 0.1831, Acc: 96.61%\n",
      "Epoch [9/30], Batch [130/259], Loss: 0.0477, Acc: 96.63%\n",
      "Epoch [9/30], Batch [140/259], Loss: 0.0735, Acc: 96.65%\n",
      "Epoch [9/30], Batch [150/259], Loss: 0.0955, Acc: 96.58%\n",
      "Epoch [9/30], Batch [160/259], Loss: 0.1035, Acc: 96.56%\n",
      "Epoch [9/30], Batch [170/259], Loss: 0.0534, Acc: 96.58%\n",
      "Epoch [9/30], Batch [180/259], Loss: 0.0343, Acc: 96.60%\n",
      "Epoch [9/30], Batch [190/259], Loss: 0.0213, Acc: 96.74%\n",
      "Epoch [9/30], Batch [200/259], Loss: 0.3079, Acc: 96.75%\n",
      "Epoch [9/30], Batch [210/259], Loss: 0.0781, Acc: 96.64%\n",
      "Epoch [9/30], Batch [220/259], Loss: 0.0263, Acc: 96.65%\n",
      "Epoch [9/30], Batch [230/259], Loss: 0.0299, Acc: 96.71%\n",
      "Epoch [9/30], Batch [240/259], Loss: 0.0713, Acc: 96.74%\n",
      "Epoch [9/30], Batch [250/259], Loss: 0.1566, Acc: 96.70%\n",
      "Epoch [9/30], Train Loss: 0.0862, Train Acc: 96.73%, Val Loss: 0.2077, Val Acc: 93.57%, Time: 86.94s\n",
      "Epoch [10/30], Batch [10/259], Loss: 0.0381, Acc: 96.25%\n",
      "Epoch [10/30], Batch [20/259], Loss: 0.0318, Acc: 95.62%\n",
      "Epoch [10/30], Batch [30/259], Loss: 0.0403, Acc: 95.62%\n",
      "Epoch [10/30], Batch [40/259], Loss: 0.0640, Acc: 95.31%\n",
      "Epoch [10/30], Batch [50/259], Loss: 0.0133, Acc: 95.00%\n",
      "Epoch [10/30], Batch [60/259], Loss: 0.0119, Acc: 95.42%\n",
      "Epoch [10/30], Batch [70/259], Loss: 0.0036, Acc: 95.54%\n",
      "Epoch [10/30], Batch [80/259], Loss: 0.1637, Acc: 95.39%\n",
      "Epoch [10/30], Batch [90/259], Loss: 0.0030, Acc: 95.69%\n",
      "Epoch [10/30], Batch [100/259], Loss: 0.0494, Acc: 95.88%\n",
      "Epoch [10/30], Batch [110/259], Loss: 0.0167, Acc: 96.14%\n",
      "Epoch [10/30], Batch [120/259], Loss: 0.0221, Acc: 96.20%\n",
      "Epoch [10/30], Batch [130/259], Loss: 0.0276, Acc: 96.30%\n",
      "Epoch [10/30], Batch [140/259], Loss: 0.0383, Acc: 96.34%\n",
      "Epoch [10/30], Batch [150/259], Loss: 0.3456, Acc: 96.29%\n",
      "Epoch [10/30], Batch [160/259], Loss: 0.4586, Acc: 96.09%\n",
      "Epoch [10/30], Batch [170/259], Loss: 0.0589, Acc: 96.14%\n",
      "Epoch [10/30], Batch [180/259], Loss: 0.4909, Acc: 95.94%\n",
      "Epoch [10/30], Batch [190/259], Loss: 0.0475, Acc: 95.79%\n",
      "Epoch [10/30], Batch [200/259], Loss: 0.1700, Acc: 95.72%\n",
      "Epoch [10/30], Batch [210/259], Loss: 0.4575, Acc: 95.51%\n",
      "Epoch [10/30], Batch [220/259], Loss: 0.1434, Acc: 95.26%\n",
      "Epoch [10/30], Batch [230/259], Loss: 0.1446, Acc: 95.22%\n",
      "Epoch [10/30], Batch [240/259], Loss: 0.3192, Acc: 95.03%\n",
      "Epoch [10/30], Batch [250/259], Loss: 0.0694, Acc: 94.92%\n",
      "Epoch [10/30], Train Loss: 0.1215, Train Acc: 94.87%, Val Loss: 0.2472, Val Acc: 91.20%, Time: 86.91s\n",
      "Epoch [11/30], Batch [10/259], Loss: 0.0624, Acc: 94.38%\n",
      "Epoch [11/30], Batch [20/259], Loss: 0.2122, Acc: 96.25%\n",
      "Epoch [11/30], Batch [30/259], Loss: 0.0201, Acc: 97.29%\n",
      "Epoch [11/30], Batch [40/259], Loss: 0.0253, Acc: 96.25%\n",
      "Epoch [11/30], Batch [50/259], Loss: 0.0912, Acc: 95.88%\n",
      "Epoch [11/30], Batch [60/259], Loss: 0.0339, Acc: 95.73%\n",
      "Epoch [11/30], Batch [70/259], Loss: 0.0151, Acc: 96.25%\n",
      "Epoch [11/30], Batch [80/259], Loss: 0.0076, Acc: 96.41%\n",
      "Epoch [11/30], Batch [90/259], Loss: 0.0022, Acc: 96.60%\n",
      "Epoch [11/30], Batch [100/259], Loss: 0.2189, Acc: 96.56%\n",
      "Epoch [11/30], Batch [110/259], Loss: 0.0792, Acc: 96.70%\n",
      "Epoch [11/30], Batch [120/259], Loss: 0.0552, Acc: 96.51%\n",
      "Epoch [11/30], Batch [130/259], Loss: 0.3968, Acc: 96.30%\n",
      "Epoch [11/30], Batch [140/259], Loss: 0.0265, Acc: 96.29%\n",
      "Epoch [11/30], Batch [150/259], Loss: 0.1236, Acc: 96.04%\n",
      "Epoch [11/30], Batch [160/259], Loss: 0.1269, Acc: 95.78%\n",
      "Epoch [11/30], Batch [170/259], Loss: 0.1871, Acc: 95.44%\n",
      "Epoch [11/30], Batch [180/259], Loss: 0.0229, Acc: 95.24%\n",
      "Epoch [11/30], Batch [190/259], Loss: 0.0463, Acc: 95.26%\n",
      "Epoch [11/30], Batch [200/259], Loss: 0.2414, Acc: 95.38%\n",
      "Epoch [11/30], Batch [210/259], Loss: 0.1378, Acc: 95.48%\n",
      "Epoch [11/30], Batch [220/259], Loss: 0.0333, Acc: 95.54%\n",
      "Epoch [11/30], Batch [230/259], Loss: 0.0047, Acc: 95.35%\n",
      "Epoch [11/30], Batch [240/259], Loss: 0.0753, Acc: 95.39%\n",
      "Epoch [11/30], Batch [250/259], Loss: 0.0529, Acc: 95.45%\n",
      "Epoch [11/30], Train Loss: 0.1158, Train Acc: 95.43%, Val Loss: 0.2755, Val Acc: 90.52%, Time: 86.95s\n",
      "Epoch [12/30], Batch [10/259], Loss: 0.0877, Acc: 93.75%\n",
      "Epoch [12/30], Batch [20/259], Loss: 0.0295, Acc: 95.31%\n",
      "Epoch [12/30], Batch [30/259], Loss: 0.1084, Acc: 95.42%\n",
      "Epoch [12/30], Batch [40/259], Loss: 0.0031, Acc: 95.94%\n",
      "Epoch [12/30], Batch [50/259], Loss: 0.1286, Acc: 96.12%\n",
      "Epoch [12/30], Batch [60/259], Loss: 0.1195, Acc: 96.04%\n",
      "Epoch [12/30], Batch [70/259], Loss: 0.0516, Acc: 96.34%\n",
      "Epoch [12/30], Batch [80/259], Loss: 0.0603, Acc: 96.33%\n",
      "Epoch [12/30], Batch [90/259], Loss: 0.0027, Acc: 96.46%\n",
      "Epoch [12/30], Batch [100/259], Loss: 0.0149, Acc: 96.62%\n",
      "Epoch [12/30], Batch [110/259], Loss: 0.0196, Acc: 96.65%\n",
      "Epoch [12/30], Batch [120/259], Loss: 0.0089, Acc: 96.77%\n",
      "Epoch [12/30], Batch [130/259], Loss: 0.1352, Acc: 96.83%\n",
      "Epoch [12/30], Batch [140/259], Loss: 0.0281, Acc: 96.70%\n",
      "Epoch [12/30], Batch [150/259], Loss: 0.1367, Acc: 96.67%\n",
      "Epoch [12/30], Batch [160/259], Loss: 0.0187, Acc: 96.84%\n",
      "Epoch [12/30], Batch [170/259], Loss: 0.0254, Acc: 96.95%\n",
      "Epoch [12/30], Batch [180/259], Loss: 0.1456, Acc: 96.91%\n",
      "Epoch [12/30], Batch [190/259], Loss: 0.0903, Acc: 96.88%\n",
      "Epoch [12/30], Batch [200/259], Loss: 0.1032, Acc: 96.88%\n",
      "Epoch [12/30], Batch [210/259], Loss: 0.2042, Acc: 96.90%\n",
      "Epoch [12/30], Batch [220/259], Loss: 0.0203, Acc: 96.96%\n",
      "Epoch [12/30], Batch [230/259], Loss: 0.1620, Acc: 96.98%\n",
      "Epoch [12/30], Batch [240/259], Loss: 0.0212, Acc: 97.03%\n",
      "Epoch [12/30], Batch [250/259], Loss: 0.1162, Acc: 97.08%\n",
      "Epoch [12/30], Train Loss: 0.0752, Train Acc: 97.10%, Val Loss: 0.1474, Val Acc: 94.42%, Time: 86.98s\n",
      "Epoch [13/30], Batch [10/259], Loss: 0.0306, Acc: 96.88%\n",
      "Epoch [13/30], Batch [20/259], Loss: 0.0156, Acc: 97.50%\n",
      "Epoch [13/30], Batch [30/259], Loss: 0.0015, Acc: 97.92%\n",
      "Epoch [13/30], Batch [40/259], Loss: 0.0087, Acc: 97.66%\n",
      "Epoch [13/30], Batch [50/259], Loss: 0.0394, Acc: 97.38%\n",
      "Epoch [13/30], Batch [60/259], Loss: 0.0671, Acc: 97.29%\n",
      "Epoch [13/30], Batch [70/259], Loss: 0.0442, Acc: 97.23%\n",
      "Epoch [13/30], Batch [80/259], Loss: 0.0042, Acc: 97.42%\n",
      "Epoch [13/30], Batch [90/259], Loss: 0.0139, Acc: 97.43%\n",
      "Epoch [13/30], Batch [100/259], Loss: 0.1711, Acc: 97.25%\n",
      "Epoch [13/30], Batch [110/259], Loss: 0.0779, Acc: 97.33%\n",
      "Epoch [13/30], Batch [120/259], Loss: 0.0777, Acc: 97.24%\n",
      "Epoch [13/30], Batch [130/259], Loss: 0.0324, Acc: 97.36%\n",
      "Epoch [13/30], Batch [140/259], Loss: 0.1501, Acc: 97.28%\n",
      "Epoch [13/30], Batch [150/259], Loss: 0.0129, Acc: 97.42%\n",
      "Epoch [13/30], Batch [160/259], Loss: 0.0117, Acc: 97.34%\n",
      "Epoch [13/30], Batch [170/259], Loss: 0.2028, Acc: 97.35%\n",
      "Epoch [13/30], Batch [180/259], Loss: 0.0007, Acc: 97.43%\n",
      "Epoch [13/30], Batch [190/259], Loss: 0.1401, Acc: 97.40%\n",
      "Epoch [13/30], Batch [200/259], Loss: 0.0011, Acc: 97.41%\n",
      "Epoch [13/30], Batch [210/259], Loss: 0.0550, Acc: 97.32%\n",
      "Epoch [13/30], Batch [220/259], Loss: 0.0295, Acc: 97.36%\n",
      "Epoch [13/30], Batch [230/259], Loss: 0.0439, Acc: 97.34%\n",
      "Epoch [13/30], Batch [240/259], Loss: 0.1054, Acc: 97.32%\n",
      "Epoch [13/30], Batch [250/259], Loss: 0.0392, Acc: 97.30%\n",
      "Epoch [13/30], Train Loss: 0.0652, Train Acc: 97.31%, Val Loss: 0.1349, Val Acc: 95.43%, Time: 86.94s\n",
      "Epoch [14/30], Batch [10/259], Loss: 0.0721, Acc: 98.12%\n",
      "Epoch [14/30], Batch [20/259], Loss: 0.0095, Acc: 98.12%\n",
      "Epoch [14/30], Batch [30/259], Loss: 0.0371, Acc: 98.33%\n",
      "Epoch [14/30], Batch [40/259], Loss: 0.0200, Acc: 98.44%\n",
      "Epoch [14/30], Batch [50/259], Loss: 0.0013, Acc: 98.62%\n",
      "Epoch [14/30], Batch [60/259], Loss: 0.2348, Acc: 98.12%\n",
      "Epoch [14/30], Batch [70/259], Loss: 0.0254, Acc: 98.21%\n",
      "Epoch [14/30], Batch [80/259], Loss: 0.1038, Acc: 98.05%\n",
      "Epoch [14/30], Batch [90/259], Loss: 0.1116, Acc: 97.92%\n",
      "Epoch [14/30], Batch [100/259], Loss: 0.0617, Acc: 97.88%\n",
      "Epoch [14/30], Batch [110/259], Loss: 0.1070, Acc: 97.61%\n",
      "Epoch [14/30], Batch [120/259], Loss: 0.1391, Acc: 97.60%\n",
      "Epoch [14/30], Batch [130/259], Loss: 0.0916, Acc: 97.55%\n",
      "Epoch [14/30], Batch [140/259], Loss: 0.1072, Acc: 97.54%\n",
      "Epoch [14/30], Batch [150/259], Loss: 0.1450, Acc: 97.54%\n",
      "Epoch [14/30], Batch [160/259], Loss: 0.0420, Acc: 97.42%\n",
      "Epoch [14/30], Batch [170/259], Loss: 0.0493, Acc: 97.35%\n",
      "Epoch [14/30], Batch [180/259], Loss: 0.1065, Acc: 97.29%\n",
      "Epoch [14/30], Batch [190/259], Loss: 0.0850, Acc: 97.30%\n",
      "Epoch [14/30], Batch [200/259], Loss: 0.0273, Acc: 97.28%\n",
      "Epoch [14/30], Batch [210/259], Loss: 0.0130, Acc: 97.35%\n",
      "Epoch [14/30], Batch [220/259], Loss: 0.1319, Acc: 97.27%\n",
      "Epoch [14/30], Batch [230/259], Loss: 0.0114, Acc: 97.36%\n",
      "Epoch [14/30], Batch [240/259], Loss: 0.1373, Acc: 97.34%\n",
      "Epoch [14/30], Batch [250/259], Loss: 0.0303, Acc: 97.33%\n",
      "Epoch [14/30], Train Loss: 0.0642, Train Acc: 97.34%, Val Loss: 0.1409, Val Acc: 95.43%, Time: 86.95s\n",
      "Epoch [15/30], Batch [10/259], Loss: 0.0325, Acc: 98.75%\n",
      "Epoch [15/30], Batch [20/259], Loss: 0.0273, Acc: 98.12%\n",
      "Epoch [15/30], Batch [30/259], Loss: 0.1989, Acc: 97.29%\n",
      "Epoch [15/30], Batch [40/259], Loss: 0.1776, Acc: 97.19%\n",
      "Epoch [15/30], Batch [50/259], Loss: 0.0257, Acc: 97.25%\n",
      "Epoch [15/30], Batch [60/259], Loss: 0.0820, Acc: 97.40%\n",
      "Epoch [15/30], Batch [70/259], Loss: 0.0006, Acc: 97.50%\n",
      "Epoch [15/30], Batch [80/259], Loss: 0.1547, Acc: 97.50%\n",
      "Epoch [15/30], Batch [90/259], Loss: 0.0223, Acc: 97.50%\n",
      "Epoch [15/30], Batch [100/259], Loss: 0.0140, Acc: 97.56%\n",
      "Epoch [15/30], Batch [110/259], Loss: 0.2131, Acc: 97.50%\n",
      "Epoch [15/30], Batch [120/259], Loss: 0.0476, Acc: 97.29%\n",
      "Epoch [15/30], Batch [130/259], Loss: 0.0213, Acc: 97.26%\n",
      "Epoch [15/30], Batch [140/259], Loss: 0.1217, Acc: 97.19%\n",
      "Epoch [15/30], Batch [150/259], Loss: 0.0879, Acc: 97.21%\n",
      "Epoch [15/30], Batch [160/259], Loss: 0.0101, Acc: 97.15%\n",
      "Epoch [15/30], Batch [170/259], Loss: 0.0409, Acc: 96.99%\n",
      "Epoch [15/30], Batch [180/259], Loss: 0.1260, Acc: 97.05%\n",
      "Epoch [15/30], Batch [190/259], Loss: 0.0007, Acc: 97.11%\n",
      "Epoch [15/30], Batch [200/259], Loss: 0.1386, Acc: 97.12%\n",
      "Epoch [15/30], Batch [210/259], Loss: 0.0200, Acc: 97.23%\n",
      "Epoch [15/30], Batch [220/259], Loss: 0.0302, Acc: 97.22%\n",
      "Epoch [15/30], Batch [230/259], Loss: 0.0390, Acc: 97.26%\n",
      "Epoch [15/30], Batch [240/259], Loss: 0.0371, Acc: 97.24%\n",
      "Epoch [15/30], Batch [250/259], Loss: 0.0113, Acc: 97.30%\n",
      "Epoch [15/30], Train Loss: 0.0618, Train Acc: 97.27%, Val Loss: 0.1426, Val Acc: 95.60%, Time: 86.97s\n",
      "Epoch [16/30], Batch [10/259], Loss: 0.0892, Acc: 98.12%\n",
      "Epoch [16/30], Batch [20/259], Loss: 0.0149, Acc: 97.81%\n",
      "Epoch [16/30], Batch [30/259], Loss: 0.0805, Acc: 97.92%\n",
      "Epoch [16/30], Batch [40/259], Loss: 0.0007, Acc: 97.19%\n",
      "Epoch [16/30], Batch [50/259], Loss: 0.0985, Acc: 97.00%\n",
      "Epoch [16/30], Batch [60/259], Loss: 0.0430, Acc: 97.08%\n",
      "Epoch [16/30], Batch [70/259], Loss: 0.0009, Acc: 97.05%\n",
      "Epoch [16/30], Batch [80/259], Loss: 0.0120, Acc: 97.42%\n",
      "Epoch [16/30], Batch [90/259], Loss: 0.1583, Acc: 97.01%\n",
      "Epoch [16/30], Batch [100/259], Loss: 0.0255, Acc: 97.19%\n",
      "Epoch [16/30], Batch [110/259], Loss: 0.1022, Acc: 97.22%\n",
      "Epoch [16/30], Batch [120/259], Loss: 0.0310, Acc: 97.34%\n",
      "Epoch [16/30], Batch [130/259], Loss: 0.1131, Acc: 97.40%\n",
      "Epoch [16/30], Batch [140/259], Loss: 0.0310, Acc: 97.41%\n",
      "Epoch [16/30], Batch [150/259], Loss: 0.2221, Acc: 97.38%\n",
      "Epoch [16/30], Batch [160/259], Loss: 0.0708, Acc: 97.27%\n",
      "Epoch [16/30], Batch [170/259], Loss: 0.0351, Acc: 97.21%\n",
      "Epoch [16/30], Batch [180/259], Loss: 0.0659, Acc: 97.19%\n",
      "Epoch [16/30], Batch [190/259], Loss: 0.0007, Acc: 97.27%\n",
      "Epoch [16/30], Batch [200/259], Loss: 0.0006, Acc: 97.31%\n",
      "Epoch [16/30], Batch [210/259], Loss: 0.0852, Acc: 97.23%\n",
      "Epoch [16/30], Batch [220/259], Loss: 0.1082, Acc: 97.19%\n",
      "Epoch [16/30], Batch [230/259], Loss: 0.0009, Acc: 97.26%\n",
      "Epoch [16/30], Batch [240/259], Loss: 0.0210, Acc: 97.34%\n",
      "Epoch [16/30], Batch [250/259], Loss: 0.0174, Acc: 97.38%\n",
      "Epoch [16/30], Train Loss: 0.0614, Train Acc: 97.31%, Val Loss: 0.1536, Val Acc: 95.43%, Time: 86.92s\n",
      "Epoch [17/30], Batch [10/259], Loss: 0.1573, Acc: 96.25%\n",
      "Epoch [17/30], Batch [20/259], Loss: 0.0009, Acc: 97.19%\n",
      "Epoch [17/30], Batch [30/259], Loss: 0.1901, Acc: 97.29%\n",
      "Epoch [17/30], Batch [40/259], Loss: 0.0719, Acc: 97.50%\n",
      "Epoch [17/30], Batch [50/259], Loss: 0.0057, Acc: 97.75%\n",
      "Epoch [17/30], Batch [60/259], Loss: 0.0068, Acc: 97.81%\n",
      "Epoch [17/30], Batch [70/259], Loss: 0.1069, Acc: 97.41%\n",
      "Epoch [17/30], Batch [80/259], Loss: 0.0422, Acc: 97.34%\n",
      "Epoch [17/30], Batch [90/259], Loss: 0.0210, Acc: 97.43%\n",
      "Epoch [17/30], Batch [100/259], Loss: 0.0404, Acc: 97.38%\n",
      "Epoch [17/30], Batch [110/259], Loss: 0.1276, Acc: 97.27%\n",
      "Epoch [17/30], Batch [120/259], Loss: 0.1208, Acc: 97.19%\n",
      "Epoch [17/30], Batch [130/259], Loss: 0.2476, Acc: 97.12%\n",
      "Epoch [17/30], Batch [140/259], Loss: 0.0798, Acc: 97.10%\n",
      "Epoch [17/30], Batch [150/259], Loss: 0.0005, Acc: 97.21%\n",
      "Epoch [17/30], Batch [160/259], Loss: 0.0201, Acc: 97.19%\n",
      "Epoch [17/30], Batch [170/259], Loss: 0.0267, Acc: 97.13%\n",
      "Epoch [17/30], Batch [180/259], Loss: 0.0311, Acc: 97.15%\n",
      "Epoch [17/30], Batch [190/259], Loss: 0.0499, Acc: 97.11%\n",
      "Epoch [17/30], Batch [200/259], Loss: 0.0100, Acc: 97.22%\n",
      "Epoch [17/30], Batch [210/259], Loss: 0.0114, Acc: 97.29%\n",
      "Epoch [17/30], Batch [220/259], Loss: 0.0120, Acc: 97.33%\n",
      "Epoch [17/30], Batch [230/259], Loss: 0.0241, Acc: 97.34%\n",
      "Epoch [17/30], Batch [240/259], Loss: 0.0551, Acc: 97.34%\n",
      "Epoch [17/30], Batch [250/259], Loss: 0.0148, Acc: 97.38%\n",
      "Epoch [17/30], Train Loss: 0.0597, Train Acc: 97.39%, Val Loss: 0.1410, Val Acc: 95.43%, Time: 86.93s\n",
      "Epoch [18/30], Batch [10/259], Loss: 0.1734, Acc: 94.38%\n",
      "Epoch [18/30], Batch [20/259], Loss: 0.0925, Acc: 95.94%\n",
      "Epoch [18/30], Batch [30/259], Loss: 0.0566, Acc: 96.88%\n",
      "Epoch [18/30], Batch [40/259], Loss: 0.0862, Acc: 96.41%\n",
      "Epoch [18/30], Batch [50/259], Loss: 0.1018, Acc: 96.75%\n",
      "Epoch [18/30], Batch [60/259], Loss: 0.2701, Acc: 96.67%\n",
      "Epoch [18/30], Batch [70/259], Loss: 0.0653, Acc: 96.70%\n",
      "Epoch [18/30], Batch [80/259], Loss: 0.0621, Acc: 96.56%\n",
      "Epoch [18/30], Batch [90/259], Loss: 0.0006, Acc: 96.67%\n",
      "Epoch [18/30], Batch [100/259], Loss: 0.0270, Acc: 96.56%\n",
      "Epoch [18/30], Batch [110/259], Loss: 0.1658, Acc: 96.70%\n",
      "Epoch [18/30], Batch [120/259], Loss: 0.0759, Acc: 96.77%\n",
      "Epoch [18/30], Batch [130/259], Loss: 0.1280, Acc: 96.88%\n",
      "Epoch [18/30], Batch [140/259], Loss: 0.0006, Acc: 96.96%\n",
      "Epoch [18/30], Batch [150/259], Loss: 0.0714, Acc: 96.96%\n",
      "Epoch [18/30], Batch [160/259], Loss: 0.0310, Acc: 96.99%\n",
      "Epoch [18/30], Batch [170/259], Loss: 0.0005, Acc: 97.10%\n",
      "Epoch [18/30], Batch [180/259], Loss: 0.0149, Acc: 97.15%\n",
      "Epoch [18/30], Batch [190/259], Loss: 0.1238, Acc: 97.17%\n",
      "Epoch [18/30], Batch [200/259], Loss: 0.0018, Acc: 97.12%\n",
      "Epoch [18/30], Batch [210/259], Loss: 0.1269, Acc: 97.11%\n",
      "Epoch [18/30], Batch [220/259], Loss: 0.0008, Acc: 97.16%\n",
      "Epoch [18/30], Batch [230/259], Loss: 0.0249, Acc: 97.26%\n",
      "Epoch [18/30], Batch [240/259], Loss: 0.0279, Acc: 97.29%\n",
      "Epoch [18/30], Batch [250/259], Loss: 0.1347, Acc: 97.30%\n",
      "Epoch [18/30], Train Loss: 0.0591, Train Acc: 97.36%, Val Loss: 0.1616, Val Acc: 95.77%, Time: 175.79s\n",
      "Epoch [19/30], Batch [10/259], Loss: 0.1267, Acc: 98.75%\n",
      "Epoch [19/30], Batch [20/259], Loss: 0.0008, Acc: 98.75%\n",
      "Epoch [19/30], Batch [30/259], Loss: 0.0246, Acc: 98.75%\n",
      "Epoch [19/30], Batch [40/259], Loss: 0.0101, Acc: 98.91%\n",
      "Epoch [19/30], Batch [50/259], Loss: 0.1293, Acc: 98.38%\n",
      "Epoch [19/30], Batch [60/259], Loss: 0.1435, Acc: 97.92%\n",
      "Epoch [19/30], Batch [70/259], Loss: 0.0252, Acc: 97.95%\n",
      "Epoch [19/30], Batch [80/259], Loss: 0.0626, Acc: 97.58%\n",
      "Epoch [19/30], Batch [90/259], Loss: 0.0568, Acc: 97.36%\n",
      "Epoch [19/30], Batch [100/259], Loss: 0.0028, Acc: 97.56%\n",
      "Epoch [19/30], Batch [110/259], Loss: 0.0008, Acc: 97.50%\n",
      "Epoch [19/30], Batch [120/259], Loss: 0.0463, Acc: 97.34%\n",
      "Epoch [19/30], Batch [130/259], Loss: 0.0608, Acc: 97.26%\n",
      "Epoch [19/30], Batch [140/259], Loss: 0.0724, Acc: 97.19%\n",
      "Epoch [19/30], Batch [150/259], Loss: 0.1154, Acc: 97.29%\n",
      "Epoch [19/30], Batch [160/259], Loss: 0.0971, Acc: 97.23%\n",
      "Epoch [19/30], Batch [170/259], Loss: 0.1206, Acc: 97.17%\n",
      "Epoch [19/30], Batch [180/259], Loss: 0.0978, Acc: 97.15%\n",
      "Epoch [19/30], Batch [190/259], Loss: 0.0975, Acc: 97.14%\n",
      "Epoch [19/30], Batch [200/259], Loss: 0.0868, Acc: 97.19%\n",
      "Epoch [19/30], Batch [210/259], Loss: 0.0915, Acc: 97.17%\n",
      "Epoch [19/30], Batch [220/259], Loss: 0.0300, Acc: 97.24%\n",
      "Epoch [19/30], Batch [230/259], Loss: 0.0270, Acc: 97.23%\n",
      "Epoch [19/30], Batch [240/259], Loss: 0.0132, Acc: 97.32%\n",
      "Epoch [19/30], Batch [250/259], Loss: 0.0005, Acc: 97.35%\n",
      "Epoch [19/30], Train Loss: 0.0583, Train Acc: 97.31%, Val Loss: 0.1956, Val Acc: 95.26%, Time: 186.74s\n",
      "Epoch [20/30], Batch [10/259], Loss: 0.0015, Acc: 96.25%\n",
      "Epoch [20/30], Batch [20/259], Loss: 0.0463, Acc: 97.19%\n",
      "Epoch [20/30], Batch [30/259], Loss: 0.0483, Acc: 97.71%\n",
      "Epoch [20/30], Batch [40/259], Loss: 0.0150, Acc: 97.81%\n",
      "Epoch [20/30], Batch [50/259], Loss: 0.1176, Acc: 97.88%\n",
      "Epoch [20/30], Batch [60/259], Loss: 0.0054, Acc: 97.92%\n",
      "Epoch [20/30], Batch [70/259], Loss: 0.0353, Acc: 97.86%\n",
      "Epoch [20/30], Batch [80/259], Loss: 0.0005, Acc: 97.89%\n",
      "Epoch [20/30], Batch [90/259], Loss: 0.1986, Acc: 97.64%\n",
      "Epoch [20/30], Batch [100/259], Loss: 0.0027, Acc: 97.75%\n",
      "Epoch [20/30], Batch [110/259], Loss: 0.0165, Acc: 97.84%\n",
      "Epoch [20/30], Batch [120/259], Loss: 0.0046, Acc: 97.81%\n",
      "Epoch [20/30], Batch [130/259], Loss: 0.0328, Acc: 97.64%\n",
      "Epoch [20/30], Batch [140/259], Loss: 0.0638, Acc: 97.41%\n",
      "Epoch [20/30], Batch [150/259], Loss: 0.0470, Acc: 97.38%\n",
      "Epoch [20/30], Batch [160/259], Loss: 0.0669, Acc: 97.15%\n",
      "Epoch [20/30], Batch [170/259], Loss: 0.0924, Acc: 96.99%\n",
      "Epoch [20/30], Batch [180/259], Loss: 0.0227, Acc: 97.12%\n",
      "Epoch [20/30], Batch [190/259], Loss: 0.0683, Acc: 97.01%\n",
      "Epoch [20/30], Batch [200/259], Loss: 0.0026, Acc: 96.97%\n",
      "Epoch [20/30], Batch [210/259], Loss: 0.0392, Acc: 96.96%\n",
      "Epoch [20/30], Batch [220/259], Loss: 0.0999, Acc: 96.93%\n",
      "Epoch [20/30], Batch [230/259], Loss: 0.0181, Acc: 97.01%\n",
      "Epoch [20/30], Batch [240/259], Loss: 0.2207, Acc: 96.93%\n",
      "Epoch [20/30], Batch [250/259], Loss: 0.0267, Acc: 97.00%\n",
      "Epoch [20/30], Train Loss: 0.0650, Train Acc: 96.97%, Val Loss: 0.1796, Val Acc: 95.09%, Time: 186.76s\n",
      "Epoch [21/30], Batch [10/259], Loss: 0.0168, Acc: 98.12%\n",
      "Epoch [21/30], Batch [20/259], Loss: 0.0821, Acc: 97.19%\n",
      "Epoch [21/30], Batch [30/259], Loss: 0.0155, Acc: 97.08%\n",
      "Epoch [21/30], Batch [40/259], Loss: 0.0494, Acc: 97.50%\n",
      "Epoch [21/30], Batch [50/259], Loss: 0.0731, Acc: 97.12%\n",
      "Epoch [21/30], Batch [60/259], Loss: 0.1296, Acc: 96.77%\n",
      "Epoch [21/30], Batch [70/259], Loss: 0.0077, Acc: 97.05%\n",
      "Epoch [21/30], Batch [80/259], Loss: 0.0005, Acc: 97.11%\n",
      "Epoch [21/30], Batch [90/259], Loss: 0.0951, Acc: 97.22%\n",
      "Epoch [21/30], Batch [100/259], Loss: 0.0017, Acc: 97.31%\n",
      "Epoch [21/30], Batch [110/259], Loss: 0.0237, Acc: 97.16%\n",
      "Epoch [21/30], Batch [120/259], Loss: 0.0012, Acc: 97.19%\n",
      "Epoch [21/30], Batch [130/259], Loss: 0.2250, Acc: 97.02%\n",
      "Epoch [21/30], Batch [140/259], Loss: 0.0773, Acc: 97.10%\n",
      "Epoch [21/30], Batch [150/259], Loss: 0.0208, Acc: 97.17%\n",
      "Epoch [21/30], Batch [160/259], Loss: 0.1831, Acc: 97.15%\n",
      "Epoch [21/30], Batch [170/259], Loss: 0.1841, Acc: 97.10%\n",
      "Epoch [21/30], Batch [180/259], Loss: 0.1831, Acc: 96.98%\n",
      "Epoch [21/30], Batch [190/259], Loss: 0.0670, Acc: 97.01%\n",
      "Epoch [21/30], Batch [200/259], Loss: 0.1167, Acc: 96.97%\n",
      "Epoch [21/30], Batch [210/259], Loss: 0.0438, Acc: 97.02%\n",
      "Epoch [21/30], Batch [220/259], Loss: 0.0007, Acc: 97.10%\n",
      "Epoch [21/30], Batch [230/259], Loss: 0.1135, Acc: 97.15%\n",
      "Epoch [21/30], Batch [240/259], Loss: 0.0015, Acc: 97.19%\n",
      "Epoch [21/30], Batch [250/259], Loss: 0.0011, Acc: 97.22%\n",
      "Epoch [21/30], Train Loss: 0.0659, Train Acc: 97.19%, Val Loss: 0.1725, Val Acc: 95.60%, Time: 187.39s\n",
      "Epoch [22/30], Batch [10/259], Loss: 0.1015, Acc: 96.25%\n",
      "Epoch [22/30], Batch [20/259], Loss: 0.0008, Acc: 96.25%\n",
      "Epoch [22/30], Batch [30/259], Loss: 0.1127, Acc: 96.46%\n",
      "Epoch [22/30], Batch [40/259], Loss: 0.0611, Acc: 97.03%\n",
      "Epoch [22/30], Batch [50/259], Loss: 0.0953, Acc: 97.00%\n",
      "Epoch [22/30], Batch [60/259], Loss: 0.0128, Acc: 97.08%\n",
      "Epoch [22/30], Batch [70/259], Loss: 0.0015, Acc: 97.23%\n",
      "Epoch [22/30], Batch [80/259], Loss: 0.0327, Acc: 97.11%\n",
      "Epoch [22/30], Batch [90/259], Loss: 0.0006, Acc: 97.29%\n",
      "Epoch [22/30], Batch [100/259], Loss: 0.0997, Acc: 97.25%\n",
      "Epoch [22/30], Batch [110/259], Loss: 0.0006, Acc: 97.27%\n",
      "Epoch [22/30], Batch [120/259], Loss: 0.1116, Acc: 97.19%\n",
      "Epoch [22/30], Batch [130/259], Loss: 0.0377, Acc: 97.31%\n",
      "Epoch [22/30], Batch [140/259], Loss: 0.0312, Acc: 97.28%\n",
      "Epoch [22/30], Batch [150/259], Loss: 0.0006, Acc: 97.25%\n",
      "Epoch [22/30], Batch [160/259], Loss: 0.1103, Acc: 97.27%\n",
      "Epoch [22/30], Batch [170/259], Loss: 0.0219, Acc: 97.24%\n",
      "Epoch [22/30], Batch [180/259], Loss: 0.0516, Acc: 97.33%\n",
      "Epoch [22/30], Batch [190/259], Loss: 0.1365, Acc: 97.37%\n",
      "Epoch [22/30], Batch [200/259], Loss: 0.0012, Acc: 97.44%\n",
      "Epoch [22/30], Batch [210/259], Loss: 0.0247, Acc: 97.50%\n",
      "Epoch [22/30], Batch [220/259], Loss: 0.0018, Acc: 97.44%\n",
      "Epoch [22/30], Batch [230/259], Loss: 0.3037, Acc: 97.39%\n",
      "Epoch [22/30], Batch [240/259], Loss: 0.0265, Acc: 97.42%\n",
      "Epoch [22/30], Batch [250/259], Loss: 0.0328, Acc: 97.38%\n",
      "Epoch [22/30], Train Loss: 0.0582, Train Acc: 97.36%, Val Loss: 0.1592, Val Acc: 95.94%, Time: 187.06s\n",
      "Epoch [23/30], Batch [10/259], Loss: 0.0047, Acc: 97.50%\n",
      "Epoch [23/30], Batch [20/259], Loss: 0.0882, Acc: 98.44%\n",
      "Epoch [23/30], Batch [30/259], Loss: 0.0098, Acc: 98.12%\n",
      "Epoch [23/30], Batch [40/259], Loss: 0.0244, Acc: 97.81%\n",
      "Epoch [23/30], Batch [50/259], Loss: 0.1569, Acc: 97.25%\n",
      "Epoch [23/30], Batch [60/259], Loss: 0.1192, Acc: 97.08%\n",
      "Epoch [23/30], Batch [70/259], Loss: 0.0707, Acc: 97.05%\n",
      "Epoch [23/30], Batch [80/259], Loss: 0.1190, Acc: 96.95%\n",
      "Epoch [23/30], Batch [90/259], Loss: 0.0838, Acc: 97.15%\n",
      "Epoch [23/30], Batch [100/259], Loss: 0.1163, Acc: 97.12%\n",
      "Epoch [23/30], Batch [110/259], Loss: 0.0025, Acc: 97.22%\n",
      "Epoch [23/30], Batch [120/259], Loss: 0.0183, Acc: 97.19%\n",
      "Epoch [23/30], Batch [130/259], Loss: 0.0006, Acc: 97.12%\n",
      "Epoch [23/30], Batch [140/259], Loss: 0.1620, Acc: 97.05%\n",
      "Epoch [23/30], Batch [150/259], Loss: 0.0724, Acc: 97.12%\n",
      "Epoch [23/30], Batch [160/259], Loss: 0.0980, Acc: 97.07%\n",
      "Epoch [23/30], Batch [170/259], Loss: 0.0007, Acc: 97.06%\n",
      "Epoch [23/30], Batch [180/259], Loss: 0.0015, Acc: 97.19%\n",
      "Epoch [23/30], Batch [190/259], Loss: 0.0502, Acc: 97.27%\n",
      "Epoch [23/30], Batch [200/259], Loss: 0.1450, Acc: 97.28%\n",
      "Epoch [23/30], Batch [210/259], Loss: 0.0182, Acc: 97.23%\n",
      "Epoch [23/30], Batch [220/259], Loss: 0.0953, Acc: 97.19%\n",
      "Epoch [23/30], Batch [230/259], Loss: 0.0266, Acc: 97.23%\n",
      "Epoch [23/30], Batch [240/259], Loss: 0.0688, Acc: 97.32%\n",
      "Epoch [23/30], Batch [250/259], Loss: 0.0797, Acc: 97.40%\n",
      "Epoch [23/30], Train Loss: 0.0560, Train Acc: 97.43%, Val Loss: 0.1911, Val Acc: 95.94%, Time: 187.33s\n",
      "Epoch [24/30], Batch [10/259], Loss: 0.0117, Acc: 98.75%\n",
      "Epoch [24/30], Batch [20/259], Loss: 0.1167, Acc: 97.50%\n",
      "Epoch [24/30], Batch [30/259], Loss: 0.1089, Acc: 97.50%\n",
      "Epoch [24/30], Batch [40/259], Loss: 0.0048, Acc: 97.81%\n",
      "Epoch [24/30], Batch [50/259], Loss: 0.1205, Acc: 97.62%\n",
      "Epoch [24/30], Batch [60/259], Loss: 0.1053, Acc: 97.19%\n",
      "Epoch [24/30], Batch [70/259], Loss: 0.0416, Acc: 97.14%\n",
      "Epoch [24/30], Batch [80/259], Loss: 0.0351, Acc: 97.27%\n",
      "Epoch [24/30], Batch [90/259], Loss: 0.0989, Acc: 97.22%\n",
      "Epoch [24/30], Batch [100/259], Loss: 0.0775, Acc: 97.19%\n",
      "Epoch [24/30], Batch [110/259], Loss: 0.0415, Acc: 97.27%\n",
      "Epoch [24/30], Batch [120/259], Loss: 0.1134, Acc: 97.34%\n",
      "Epoch [24/30], Batch [130/259], Loss: 0.0401, Acc: 97.45%\n",
      "Epoch [24/30], Batch [140/259], Loss: 0.0229, Acc: 97.46%\n",
      "Epoch [24/30], Batch [150/259], Loss: 0.0010, Acc: 97.58%\n",
      "Epoch [24/30], Batch [160/259], Loss: 0.0213, Acc: 97.54%\n",
      "Epoch [24/30], Batch [170/259], Loss: 0.0716, Acc: 97.46%\n",
      "Epoch [24/30], Batch [180/259], Loss: 0.0008, Acc: 97.47%\n",
      "Epoch [24/30], Batch [190/259], Loss: 0.0286, Acc: 97.43%\n",
      "Epoch [24/30], Batch [200/259], Loss: 0.0329, Acc: 97.47%\n",
      "Epoch [24/30], Batch [210/259], Loss: 0.1167, Acc: 97.50%\n",
      "Epoch [24/30], Batch [220/259], Loss: 0.0198, Acc: 97.50%\n",
      "Epoch [24/30], Batch [230/259], Loss: 0.0751, Acc: 97.31%\n",
      "Epoch [24/30], Batch [240/259], Loss: 0.0368, Acc: 97.40%\n",
      "Epoch [24/30], Batch [250/259], Loss: 0.0291, Acc: 97.38%\n",
      "Epoch [24/30], Train Loss: 0.0547, Train Acc: 97.39%, Val Loss: 0.2179, Val Acc: 95.77%, Time: 187.27s\n",
      "Epoch [25/30], Batch [10/259], Loss: 0.0003, Acc: 96.88%\n",
      "Epoch [25/30], Batch [20/259], Loss: 0.0006, Acc: 97.81%\n",
      "Epoch [25/30], Batch [30/259], Loss: 0.0005, Acc: 97.92%\n",
      "Epoch [25/30], Batch [40/259], Loss: 0.1769, Acc: 97.50%\n",
      "Epoch [25/30], Batch [50/259], Loss: 0.0182, Acc: 97.50%\n",
      "Epoch [25/30], Batch [60/259], Loss: 0.0262, Acc: 97.29%\n",
      "Epoch [25/30], Batch [70/259], Loss: 0.0200, Acc: 97.41%\n",
      "Epoch [25/30], Batch [80/259], Loss: 0.1660, Acc: 97.27%\n",
      "Epoch [25/30], Batch [90/259], Loss: 0.0005, Acc: 97.29%\n",
      "Epoch [25/30], Batch [100/259], Loss: 0.1200, Acc: 97.00%\n",
      "Epoch [25/30], Batch [110/259], Loss: 0.1111, Acc: 97.05%\n",
      "Epoch [25/30], Batch [120/259], Loss: 0.0549, Acc: 96.93%\n",
      "Epoch [25/30], Batch [130/259], Loss: 0.0656, Acc: 96.92%\n",
      "Epoch [25/30], Batch [140/259], Loss: 0.0272, Acc: 96.96%\n",
      "Epoch [25/30], Batch [150/259], Loss: 0.0344, Acc: 96.88%\n",
      "Epoch [25/30], Batch [160/259], Loss: 0.0004, Acc: 96.95%\n",
      "Epoch [25/30], Batch [170/259], Loss: 0.0287, Acc: 97.06%\n",
      "Epoch [25/30], Batch [180/259], Loss: 0.0658, Acc: 97.05%\n",
      "Epoch [25/30], Batch [190/259], Loss: 0.0326, Acc: 97.17%\n",
      "Epoch [25/30], Batch [200/259], Loss: 0.1028, Acc: 97.22%\n",
      "Epoch [25/30], Batch [210/259], Loss: 0.0004, Acc: 97.23%\n",
      "Epoch [25/30], Batch [220/259], Loss: 0.1108, Acc: 97.19%\n",
      "Epoch [25/30], Batch [230/259], Loss: 0.0203, Acc: 97.26%\n",
      "Epoch [25/30], Batch [240/259], Loss: 0.0152, Acc: 97.29%\n",
      "Epoch [25/30], Batch [250/259], Loss: 0.1105, Acc: 97.35%\n",
      "Epoch [25/30], Train Loss: 0.0532, Train Acc: 97.39%, Val Loss: 0.2341, Val Acc: 95.77%, Time: 187.35s\n",
      "Epoch [26/30], Batch [10/259], Loss: 0.1077, Acc: 98.12%\n",
      "Epoch [26/30], Batch [20/259], Loss: 0.0185, Acc: 97.81%\n",
      "Epoch [26/30], Batch [30/259], Loss: 0.0008, Acc: 97.50%\n",
      "Epoch [26/30], Batch [40/259], Loss: 0.0551, Acc: 97.66%\n",
      "Epoch [26/30], Batch [50/259], Loss: 0.1181, Acc: 97.75%\n",
      "Epoch [26/30], Batch [60/259], Loss: 0.0006, Acc: 97.08%\n",
      "Epoch [26/30], Batch [70/259], Loss: 0.1192, Acc: 97.41%\n",
      "Epoch [26/30], Batch [80/259], Loss: 0.0203, Acc: 97.11%\n",
      "Epoch [26/30], Batch [90/259], Loss: 0.0017, Acc: 97.22%\n",
      "Epoch [26/30], Batch [100/259], Loss: 0.1008, Acc: 97.31%\n",
      "Epoch [26/30], Batch [110/259], Loss: 0.0852, Acc: 97.27%\n",
      "Epoch [26/30], Batch [120/259], Loss: 0.1746, Acc: 97.19%\n",
      "Epoch [26/30], Batch [130/259], Loss: 0.0270, Acc: 97.21%\n",
      "Epoch [26/30], Batch [140/259], Loss: 0.0279, Acc: 97.32%\n",
      "Epoch [26/30], Batch [150/259], Loss: 0.0016, Acc: 97.33%\n",
      "Epoch [26/30], Batch [160/259], Loss: 0.0413, Acc: 97.38%\n",
      "Epoch [26/30], Batch [170/259], Loss: 0.0385, Acc: 97.46%\n",
      "Epoch [26/30], Batch [180/259], Loss: 0.0009, Acc: 97.33%\n",
      "Epoch [26/30], Batch [190/259], Loss: 0.0005, Acc: 97.37%\n",
      "Epoch [26/30], Batch [200/259], Loss: 0.1151, Acc: 97.41%\n",
      "Epoch [26/30], Batch [210/259], Loss: 0.0270, Acc: 97.41%\n",
      "Epoch [26/30], Batch [220/259], Loss: 0.0280, Acc: 97.44%\n",
      "Epoch [26/30], Batch [230/259], Loss: 0.1011, Acc: 97.31%\n",
      "Epoch [26/30], Batch [240/259], Loss: 0.0005, Acc: 97.32%\n",
      "Epoch [26/30], Batch [250/259], Loss: 0.0244, Acc: 97.33%\n",
      "Epoch [26/30], Train Loss: 0.0533, Train Acc: 97.39%, Val Loss: 0.2381, Val Acc: 95.77%, Time: 187.28s\n",
      "Epoch [27/30], Batch [10/259], Loss: 0.0212, Acc: 99.38%\n",
      "Epoch [27/30], Batch [20/259], Loss: 0.0429, Acc: 99.38%\n",
      "Epoch [27/30], Batch [30/259], Loss: 0.0005, Acc: 98.54%\n",
      "Epoch [27/30], Batch [40/259], Loss: 0.0722, Acc: 98.28%\n",
      "Epoch [27/30], Batch [50/259], Loss: 0.0214, Acc: 98.12%\n",
      "Epoch [27/30], Batch [60/259], Loss: 0.2035, Acc: 97.81%\n",
      "Epoch [27/30], Batch [70/259], Loss: 0.0656, Acc: 97.86%\n",
      "Epoch [27/30], Batch [80/259], Loss: 0.1145, Acc: 97.73%\n",
      "Epoch [27/30], Batch [90/259], Loss: 0.0212, Acc: 97.92%\n",
      "Epoch [27/30], Batch [100/259], Loss: 0.0266, Acc: 97.94%\n",
      "Epoch [27/30], Batch [110/259], Loss: 0.1744, Acc: 97.84%\n",
      "Epoch [27/30], Batch [120/259], Loss: 0.0011, Acc: 97.86%\n",
      "Epoch [27/30], Batch [130/259], Loss: 0.0973, Acc: 97.84%\n",
      "Epoch [27/30], Batch [140/259], Loss: 0.0693, Acc: 97.77%\n",
      "Epoch [27/30], Batch [150/259], Loss: 0.1170, Acc: 97.67%\n",
      "Epoch [27/30], Batch [160/259], Loss: 0.0008, Acc: 97.70%\n",
      "Epoch [27/30], Batch [170/259], Loss: 0.0279, Acc: 97.72%\n",
      "Epoch [27/30], Batch [180/259], Loss: 0.0005, Acc: 97.74%\n",
      "Epoch [27/30], Batch [190/259], Loss: 0.0675, Acc: 97.66%\n",
      "Epoch [27/30], Batch [200/259], Loss: 0.0004, Acc: 97.69%\n",
      "Epoch [27/30], Batch [210/259], Loss: 0.0935, Acc: 97.59%\n",
      "Epoch [27/30], Batch [220/259], Loss: 0.0624, Acc: 97.50%\n",
      "Epoch [27/30], Batch [230/259], Loss: 0.0618, Acc: 97.50%\n",
      "Epoch [27/30], Batch [240/259], Loss: 0.0105, Acc: 97.47%\n",
      "Epoch [27/30], Batch [250/259], Loss: 0.1428, Acc: 97.35%\n",
      "Epoch [27/30], Train Loss: 0.0530, Train Acc: 97.39%, Val Loss: 0.2431, Val Acc: 95.77%, Time: 185.80s\n",
      "Epoch [28/30], Batch [10/259], Loss: 0.0004, Acc: 96.25%\n",
      "Epoch [28/30], Batch [20/259], Loss: 0.0680, Acc: 96.25%\n",
      "Epoch [28/30], Batch [30/259], Loss: 0.0279, Acc: 96.88%\n",
      "Epoch [28/30], Batch [40/259], Loss: 0.0253, Acc: 97.19%\n",
      "Epoch [28/30], Batch [50/259], Loss: 0.0988, Acc: 97.25%\n",
      "Epoch [28/30], Batch [60/259], Loss: 0.0004, Acc: 97.29%\n",
      "Epoch [28/30], Batch [70/259], Loss: 0.0395, Acc: 97.41%\n",
      "Epoch [28/30], Batch [80/259], Loss: 0.1018, Acc: 97.50%\n",
      "Epoch [28/30], Batch [90/259], Loss: 0.0604, Acc: 97.43%\n",
      "Epoch [28/30], Batch [100/259], Loss: 0.0261, Acc: 97.44%\n",
      "Epoch [28/30], Batch [110/259], Loss: 0.0929, Acc: 97.27%\n",
      "Epoch [28/30], Batch [120/259], Loss: 0.0547, Acc: 97.19%\n",
      "Epoch [28/30], Batch [130/259], Loss: 0.0007, Acc: 97.26%\n",
      "Epoch [28/30], Batch [140/259], Loss: 0.0749, Acc: 97.23%\n",
      "Epoch [28/30], Batch [150/259], Loss: 0.0026, Acc: 97.29%\n",
      "Epoch [28/30], Batch [160/259], Loss: 0.0017, Acc: 97.34%\n",
      "Epoch [28/30], Batch [170/259], Loss: 0.0660, Acc: 97.32%\n",
      "Epoch [28/30], Batch [180/259], Loss: 0.0005, Acc: 97.29%\n",
      "Epoch [28/30], Batch [190/259], Loss: 0.1234, Acc: 97.30%\n",
      "Epoch [28/30], Batch [200/259], Loss: 0.0389, Acc: 97.31%\n",
      "Epoch [28/30], Batch [210/259], Loss: 0.1095, Acc: 97.29%\n",
      "Epoch [28/30], Batch [220/259], Loss: 0.0446, Acc: 97.41%\n",
      "Epoch [28/30], Batch [230/259], Loss: 0.1195, Acc: 97.45%\n",
      "Epoch [28/30], Batch [240/259], Loss: 0.0012, Acc: 97.37%\n",
      "Epoch [28/30], Batch [250/259], Loss: 0.1511, Acc: 97.38%\n",
      "Epoch [28/30], Train Loss: 0.0523, Train Acc: 97.41%, Val Loss: 0.2646, Val Acc: 95.77%, Time: 187.11s\n",
      "Epoch [29/30], Batch [10/259], Loss: 0.0252, Acc: 97.50%\n",
      "Epoch [29/30], Batch [20/259], Loss: 0.0247, Acc: 98.12%\n",
      "Epoch [29/30], Batch [30/259], Loss: 0.1619, Acc: 97.50%\n",
      "Epoch [29/30], Batch [40/259], Loss: 0.0008, Acc: 97.81%\n",
      "Epoch [29/30], Batch [50/259], Loss: 0.1250, Acc: 97.75%\n",
      "Epoch [29/30], Batch [60/259], Loss: 0.0827, Acc: 97.81%\n",
      "Epoch [29/30], Batch [70/259], Loss: 0.0702, Acc: 97.95%\n",
      "Epoch [29/30], Batch [80/259], Loss: 0.0974, Acc: 97.81%\n",
      "Epoch [29/30], Batch [90/259], Loss: 0.0225, Acc: 97.78%\n",
      "Epoch [29/30], Batch [100/259], Loss: 0.0211, Acc: 97.88%\n",
      "Epoch [29/30], Batch [110/259], Loss: 0.0212, Acc: 97.90%\n",
      "Epoch [29/30], Batch [120/259], Loss: 0.0486, Acc: 97.81%\n",
      "Epoch [29/30], Batch [130/259], Loss: 0.0006, Acc: 97.64%\n",
      "Epoch [29/30], Batch [140/259], Loss: 0.0697, Acc: 97.37%\n",
      "Epoch [29/30], Batch [150/259], Loss: 0.0580, Acc: 97.38%\n",
      "Epoch [29/30], Batch [160/259], Loss: 0.0814, Acc: 97.30%\n",
      "Epoch [29/30], Batch [170/259], Loss: 0.0424, Acc: 97.24%\n",
      "Epoch [29/30], Batch [180/259], Loss: 0.0690, Acc: 97.26%\n",
      "Epoch [29/30], Batch [190/259], Loss: 0.0039, Acc: 97.37%\n",
      "Epoch [29/30], Batch [200/259], Loss: 0.0282, Acc: 97.44%\n",
      "Epoch [29/30], Batch [210/259], Loss: 0.1425, Acc: 97.44%\n",
      "Epoch [29/30], Batch [220/259], Loss: 0.0003, Acc: 97.39%\n",
      "Epoch [29/30], Batch [230/259], Loss: 0.1135, Acc: 97.39%\n",
      "Epoch [29/30], Batch [240/259], Loss: 0.0199, Acc: 97.45%\n",
      "Epoch [29/30], Batch [250/259], Loss: 0.0840, Acc: 97.38%\n",
      "Epoch [29/30], Train Loss: 0.0519, Train Acc: 97.39%, Val Loss: 0.2626, Val Acc: 95.77%, Time: 186.99s\n",
      "Epoch [30/30], Batch [10/259], Loss: 0.0675, Acc: 95.62%\n",
      "Epoch [30/30], Batch [20/259], Loss: 0.0658, Acc: 96.25%\n",
      "Epoch [30/30], Batch [30/259], Loss: 0.1068, Acc: 96.88%\n",
      "Epoch [30/30], Batch [40/259], Loss: 0.0004, Acc: 97.03%\n",
      "Epoch [30/30], Batch [50/259], Loss: 0.0476, Acc: 96.62%\n",
      "Epoch [30/30], Batch [60/259], Loss: 0.0202, Acc: 96.98%\n",
      "Epoch [30/30], Batch [70/259], Loss: 0.0006, Acc: 96.96%\n",
      "Epoch [30/30], Batch [80/259], Loss: 0.0006, Acc: 97.34%\n",
      "Epoch [30/30], Batch [90/259], Loss: 0.0853, Acc: 97.43%\n",
      "Epoch [30/30], Batch [100/259], Loss: 0.0256, Acc: 97.44%\n",
      "Epoch [30/30], Batch [110/259], Loss: 0.0261, Acc: 97.27%\n",
      "Epoch [30/30], Batch [120/259], Loss: 0.0797, Acc: 97.34%\n",
      "Epoch [30/30], Batch [130/259], Loss: 0.0953, Acc: 97.40%\n",
      "Epoch [30/30], Batch [140/259], Loss: 0.0181, Acc: 97.50%\n",
      "Epoch [30/30], Batch [150/259], Loss: 0.0005, Acc: 97.54%\n",
      "Epoch [30/30], Batch [160/259], Loss: 0.0208, Acc: 97.42%\n",
      "Epoch [30/30], Batch [170/259], Loss: 0.0107, Acc: 97.46%\n",
      "Epoch [30/30], Batch [180/259], Loss: 0.1290, Acc: 97.43%\n",
      "Epoch [30/30], Batch [190/259], Loss: 0.0003, Acc: 97.47%\n",
      "Epoch [30/30], Batch [200/259], Loss: 0.0230, Acc: 97.41%\n",
      "Epoch [30/30], Batch [210/259], Loss: 0.0986, Acc: 97.29%\n",
      "Epoch [30/30], Batch [220/259], Loss: 0.0199, Acc: 97.30%\n",
      "Epoch [30/30], Batch [230/259], Loss: 0.0652, Acc: 97.31%\n",
      "Epoch [30/30], Batch [240/259], Loss: 0.1292, Acc: 97.34%\n",
      "Epoch [30/30], Batch [250/259], Loss: 0.0266, Acc: 97.40%\n",
      "Epoch [30/30], Train Loss: 0.0519, Train Acc: 97.39%, Val Loss: 0.2638, Val Acc: 95.77%, Time: 187.17s\n",
      "\n",
      "Loading best model for evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_737855/2098237430.py:378: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 21 with validation accuracy 95.94%\n",
      "\n",
      "Evaluating best model...\n",
      "Test Accuracy: 94.83%\n",
      "Ambulance Left Accuracy: 97.39%\n",
      "Ambulance Middle Accuracy: 100.00%\n",
      "Ambulance Right Accuracy: 96.19%\n",
      "Car Horn Left Accuracy: 98.91%\n",
      "Car Horn Middle Accuracy: 100.00%\n",
      "Car Horn Right Accuracy: 98.95%\n",
      "Fire Truck Left Accuracy: 93.83%\n",
      "Fire Truck Middle Accuracy: 100.00%\n",
      "Fire Truck Right Accuracy: 60.67%\n",
      "Police Car Left Accuracy: 89.72%\n",
      "Police Car Middle Accuracy: 99.08%\n",
      "Police Car Right Accuracy: 100.00%\n",
      "Bike Left Accuracy: 100.00%\n",
      "Bike Right Accuracy: 100.00%\n",
      "Bike Middle Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import time\n",
    "\n",
    "# Early stopping class remains unchanged\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "class DirectionalSoundDataset(Dataset):\n",
    "    def __init__(self, base_dir, transform=None, target_size=(224, 224)):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Updated class mapping to include bike categories\n",
    "        self.class_to_idx = {\n",
    "            'ambulance_L': 0,\n",
    "            'ambulance_M': 1,\n",
    "            'ambulance_R': 2,\n",
    "            'carhorns_L': 3,\n",
    "            'carhorns_M': 4,\n",
    "            'carhorns_R': 5,\n",
    "            'FireTruck_L': 6,\n",
    "            'FireTruck_M': 7,\n",
    "            'FireTruck_R': 8,\n",
    "            'policecar_L': 9,\n",
    "            'policecar_M': 10,\n",
    "            'policecar_R': 11,\n",
    "            'Bike_L': 12,\n",
    "            'Bike_R': 13,\n",
    "            'Bike_B': 14\n",
    "        }\n",
    "        \n",
    "        # Rest of the DirectionalSoundDataset implementation remains the same\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name in self.class_to_idx.keys():\n",
    "            class_dir = self.base_dir / class_name\n",
    "            if class_dir.exists():\n",
    "                class_files = list(class_dir.glob(f\"{class_name}_*.png\"))\n",
    "                self.files.extend(class_files)\n",
    "                self.labels.extend([self.class_to_idx[class_name]] * len(class_files))\n",
    "        \n",
    "        if len(self.files) == 0:\n",
    "            raise RuntimeError(f\"No spectrogram files found in {base_dir}\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        spectrogram = Image.open(img_path).convert('RGB')\n",
    "        if spectrogram.size != self.target_size:\n",
    "            spectrogram = spectrogram.resize(self.target_size)\n",
    "        \n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        return spectrogram, label\n",
    "\n",
    "def create_data_loaders(base_dir, batch_size=16):\n",
    "    \"\"\"\n",
    "    Create data loaders with a 70-10-20 split for training, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): Path to the dataset directory\n",
    "        batch_size (int): Batch size for the data loaders\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Create full dataset\n",
    "    full_dataset = DirectionalSoundDataset(\n",
    "        base_dir=base_dir,\n",
    "        transform=transform,\n",
    "        target_size=(224, 224)\n",
    "    )\n",
    "    \n",
    "    # Calculate sizes\n",
    "    total_size = len(full_dataset)\n",
    "    test_size = 0.2  # 20% for test set\n",
    "    val_size = 0.1   # 10% for validation set\n",
    "    train_size = 0.7 # 70% for training set\n",
    "    \n",
    "    # Create indices for the splits\n",
    "    indices = list(range(total_size))\n",
    "    \n",
    "    # First split: separate out test set (20%)\n",
    "    train_val_idx, test_idx = train_test_split(\n",
    "        indices,\n",
    "        test_size=test_size,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Second split: separate training (70%) and validation (10%) from the remaining 80%\n",
    "    # To get 10% validation from the remaining 80%, we need val_size/(train_size + val_size) = 0.125\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        train_val_idx,\n",
    "        test_size=val_size/(train_size + val_size),\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Verify split sizes\n",
    "    print(f\"Total dataset size: {total_size}\")\n",
    "    print(f\"Training set size: {len(train_idx)} ({len(train_idx)/total_size*100:.1f}%)\")\n",
    "    print(f\"Validation set size: {len(val_idx)} ({len(val_idx)/total_size*100:.1f}%)\")\n",
    "    print(f\"Test set size: {len(test_idx)} ({len(test_idx)/total_size*100:.1f}%)\")\n",
    "    \n",
    "    # Create samplers\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        full_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        full_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=val_sampler,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        full_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=test_sampler,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Optional: Function to verify the split distribution across classes\n",
    "def verify_split_distribution(train_loader, val_loader, test_loader, num_classes=15):\n",
    "    \"\"\"\n",
    "    Verify the distribution of classes across the different splits.\n",
    "    \"\"\"\n",
    "    def count_samples_per_class(loader):\n",
    "        class_counts = torch.zeros(num_classes)\n",
    "        for _, labels in loader:\n",
    "            for label in labels:\n",
    "                class_counts[label] += 1\n",
    "        return class_counts\n",
    "    \n",
    "    train_dist = count_samples_per_class(train_loader)\n",
    "    val_dist = count_samples_per_class(val_loader)\n",
    "    test_dist = count_samples_per_class(test_loader)\n",
    "    \n",
    "    print(\"\\nClass distribution in splits:\")\n",
    "    print(f\"{'Class':<15} {'Train':>10} {'Val':>10} {'Test':>10}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        total = train_dist[i] + val_dist[i] + test_dist[i]\n",
    "        if total > 0:  # Only print if class has samples\n",
    "            train_pct = train_dist[i] / total * 100\n",
    "            val_pct = val_dist[i] / total * 100\n",
    "            test_pct = test_dist[i] / total * 100\n",
    "            print(f\"{i:<15} {train_pct:>9.1f}% {val_pct:>9.1f}% {test_pct:>9.1f}%\")\n",
    "\n",
    "class DirectionalSoundViT(nn.Module):\n",
    "    def __init__(self, num_classes=15):  # Updated to 15 classes to include bike categories\n",
    "        super().__init__()\n",
    "        self.vit = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        num_features = self.vit.heads.head.in_features\n",
    "        self.vit.heads.head = nn.Linear(num_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.vit(x)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=30):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Initialize optimizer and loss\n",
    "        optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Create directory for saving checkpoints\n",
    "        save_dir = Path('model_checkpoints')\n",
    "        save_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        \n",
    "        try:\n",
    "            for epoch in range(num_epochs):\n",
    "                # Training phase\n",
    "                model.train()\n",
    "                total_loss = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                start_time = time.time()\n",
    "                \n",
    "                for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
    "                    try:\n",
    "                        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                        \n",
    "                        outputs = model(spectrograms)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        total_loss += loss.item()\n",
    "                        \n",
    "                        _, predicted = outputs.max(1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += predicted.eq(labels).sum().item()\n",
    "                        \n",
    "                        if (batch_idx + 1) % 10 == 0:\n",
    "                            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                                f'Batch [{batch_idx+1}/{len(train_loader)}], '\n",
    "                                f'Loss: {loss.item():.4f}, '\n",
    "                                f'Acc: {100.*correct/total:.2f}%')\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                train_acc = 100.*correct/total\n",
    "                avg_loss = total_loss / len(train_loader)\n",
    "                epoch_time = time.time() - start_time\n",
    "                \n",
    "                # Validation phase\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for spectrograms, labels in val_loader:\n",
    "                        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                        outputs = model(spectrograms)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                val_acc = 100.*correct/total\n",
    "                val_loss = val_loss / len(val_loader)\n",
    "                \n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                    f'Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "                    f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, '\n",
    "                    f'Time: {epoch_time:.2f}s')\n",
    "                \n",
    "                # Save best model\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'val_acc': val_acc,\n",
    "                    }, save_dir / 'updated_best_model.pth')\n",
    "                \n",
    "                scheduler.step()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Training error: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "# Training and evaluation functions remain largely the same, but with updated class names\n",
    "def evaluate_model(model, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for spectrograms, labels in test_loader:\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            outputs = model(spectrograms)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    # Updated class names to include bike categories\n",
    "    class_names = [\n",
    "        'Ambulance Left', 'Ambulance Middle', 'Ambulance Right',\n",
    "        'Car Horn Left', 'Car Horn Middle', 'Car Horn Right',\n",
    "        'Fire Truck Left', 'Fire Truck Middle', 'Fire Truck Right',\n",
    "        'Police Car Left', 'Police Car Middle', 'Police Car Right',\n",
    "        'Bike Left', 'Bike Right', 'Bike Middle'\n",
    "    ]\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_mask = (all_labels == i)\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_acc = 100 * np.sum((all_predictions == i) & class_mask) / np.sum(class_mask)\n",
    "            print(f'{class_name} Accuracy: {class_acc:.2f}%')\n",
    "    \n",
    "    return accuracy, all_predictions, all_labels\n",
    "def load_best_model(model, filepath):\n",
    "    \"\"\"Load the best model weights\"\"\"\n",
    "    if not Path(filepath).exists():\n",
    "        raise FileNotFoundError(f\"No model checkpoint found at {filepath}\")\n",
    "    \n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded model from epoch {checkpoint['epoch']} with validation accuracy {checkpoint['val_acc']:.2f}%\")\n",
    "    return model, checkpoint['epoch'], checkpoint['val_acc']\n",
    "\n",
    "def save_model(model, optimizer, epoch, val_acc, filename):\n",
    "    \"\"\"Save model checkpoint with all necessary state information\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_acc': val_acc,\n",
    "    }, filename)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "def inference(model, image_path, device=None):\n",
    "    \"\"\"Run inference on a single image\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Updated class names to include bike categories\n",
    "    class_names = [\n",
    "        'Ambulance Left', 'Ambulance Middle', 'Ambulance Right',\n",
    "        'Car Horn Left', 'Car Horn Middle', 'Car Horn Right',\n",
    "        'Fire Truck Left', 'Fire Truck Middle', 'Fire Truck Right',\n",
    "        'Police Car Left', 'Police Car Middle', 'Police Car Right',\n",
    "        'Bike Left', 'Bike Right', 'Bike Middle'\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "    return {\n",
    "        'predicted_class': class_names[predicted_class],\n",
    "        'confidence': confidence * 100,\n",
    "        'all_probabilities': {\n",
    "            class_name: prob.item() * 100 \n",
    "            for class_name, prob in zip(class_names, probabilities[0])\n",
    "        }\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"Dataset of warning sound types and source directions\"\n",
    "    checkpoint_dir = Path('model_checkpoints')\n",
    "    best_model_path = checkpoint_dir / 'updated_best_model.pth'\n",
    "    \n",
    "    train_loader, val_loader, test_loader = create_data_loaders(base_dir, batch_size=16)\n",
    "    \n",
    "    # Verify the split distribution\n",
    "    verify_split_distribution(train_loader, val_loader, test_loader)    \n",
    "    # Create model with 15 classes (including bike categories)\n",
    "    model = DirectionalSoundViT(num_classes=15)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_loader, val_loader, num_epochs=30)\n",
    "    \n",
    "    print(\"\\nLoading best model for evaluation...\")\n",
    "    model, best_epoch, best_val_acc = load_best_model(model, best_model_path)\n",
    "    \n",
    "    print(\"\\nEvaluating best model...\")\n",
    "    accuracy, predictions, labels = evaluate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
