{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/30], Batch [10/116], Loss: 2.3690, Acc: 11.25%\n",
      "Epoch [1/30], Batch [20/116], Loss: 1.5902, Acc: 19.53%\n",
      "Epoch [1/30], Batch [30/116], Loss: 1.5932, Acc: 21.56%\n",
      "Epoch [1/30], Batch [40/116], Loss: 1.4731, Acc: 24.22%\n",
      "Epoch [1/30], Batch [50/116], Loss: 1.1913, Acc: 25.31%\n",
      "Epoch [1/30], Batch [60/116], Loss: 1.1824, Acc: 26.72%\n",
      "Epoch [1/30], Batch [70/116], Loss: 1.1839, Acc: 29.06%\n",
      "Epoch [1/30], Batch [80/116], Loss: 1.3216, Acc: 29.65%\n",
      "Epoch [1/30], Batch [90/116], Loss: 1.0653, Acc: 30.17%\n",
      "Epoch [1/30], Batch [100/116], Loss: 1.0722, Acc: 30.81%\n",
      "Epoch [1/30], Batch [110/116], Loss: 1.0634, Acc: 31.70%\n",
      "Epoch [1/30], Train Loss: 1.4147, Train Acc: 32.49%, Val Loss: 1.1910, Val Acc: 37.31%, Time: 71.76s\n",
      "Epoch [2/30], Batch [10/116], Loss: 1.1309, Acc: 44.38%\n",
      "Epoch [2/30], Batch [20/116], Loss: 0.9869, Acc: 47.03%\n",
      "Epoch [2/30], Batch [30/116], Loss: 1.0407, Acc: 47.60%\n",
      "Epoch [2/30], Batch [40/116], Loss: 0.8112, Acc: 48.98%\n",
      "Epoch [2/30], Batch [50/116], Loss: 0.8217, Acc: 50.44%\n",
      "Epoch [2/30], Batch [60/116], Loss: 0.8230, Acc: 51.77%\n",
      "Epoch [2/30], Batch [70/116], Loss: 0.8200, Acc: 52.63%\n",
      "Epoch [2/30], Batch [80/116], Loss: 0.8492, Acc: 54.14%\n",
      "Epoch [2/30], Batch [90/116], Loss: 1.0303, Acc: 55.17%\n",
      "Epoch [2/30], Batch [100/116], Loss: 0.6921, Acc: 56.16%\n",
      "Epoch [2/30], Batch [110/116], Loss: 0.5621, Acc: 57.27%\n",
      "Epoch [2/30], Train Loss: 0.8893, Train Acc: 58.17%, Val Loss: 0.7235, Val Acc: 66.29%, Time: 72.37s\n",
      "Epoch [3/30], Batch [10/116], Loss: 0.5639, Acc: 78.44%\n",
      "Epoch [3/30], Batch [20/116], Loss: 0.3674, Acc: 80.78%\n",
      "Epoch [3/30], Batch [30/116], Loss: 0.6164, Acc: 79.17%\n",
      "Epoch [3/30], Batch [40/116], Loss: 0.3619, Acc: 79.53%\n",
      "Epoch [3/30], Batch [50/116], Loss: 0.6239, Acc: 80.12%\n",
      "Epoch [3/30], Batch [60/116], Loss: 0.7203, Acc: 80.05%\n",
      "Epoch [3/30], Batch [70/116], Loss: 0.4168, Acc: 80.40%\n",
      "Epoch [3/30], Batch [80/116], Loss: 0.2837, Acc: 80.98%\n",
      "Epoch [3/30], Batch [90/116], Loss: 0.1760, Acc: 81.67%\n",
      "Epoch [3/30], Batch [100/116], Loss: 0.5114, Acc: 81.62%\n",
      "Epoch [3/30], Batch [110/116], Loss: 0.1790, Acc: 81.96%\n",
      "Epoch [3/30], Train Loss: 0.4475, Train Acc: 82.33%, Val Loss: 0.5584, Val Acc: 77.65%, Time: 72.72s\n",
      "Epoch [4/30], Batch [10/116], Loss: 0.4331, Acc: 89.38%\n",
      "Epoch [4/30], Batch [20/116], Loss: 0.3444, Acc: 88.91%\n",
      "Epoch [4/30], Batch [30/116], Loss: 0.2406, Acc: 89.69%\n",
      "Epoch [4/30], Batch [40/116], Loss: 0.0688, Acc: 90.62%\n",
      "Epoch [4/30], Batch [50/116], Loss: 0.0384, Acc: 91.62%\n",
      "Epoch [4/30], Batch [60/116], Loss: 0.0593, Acc: 91.61%\n",
      "Epoch [4/30], Batch [70/116], Loss: 0.0934, Acc: 91.70%\n",
      "Epoch [4/30], Batch [80/116], Loss: 0.0227, Acc: 92.11%\n",
      "Epoch [4/30], Batch [90/116], Loss: 0.1016, Acc: 92.26%\n",
      "Epoch [4/30], Batch [100/116], Loss: 0.4966, Acc: 92.50%\n",
      "Epoch [4/30], Batch [110/116], Loss: 0.0529, Acc: 92.84%\n",
      "Epoch [4/30], Train Loss: 0.2169, Train Acc: 92.83%, Val Loss: 0.3726, Val Acc: 86.55%, Time: 72.84s\n",
      "Epoch [5/30], Batch [10/116], Loss: 0.3657, Acc: 92.81%\n",
      "Epoch [5/30], Batch [20/116], Loss: 0.1975, Acc: 95.47%\n",
      "Epoch [5/30], Batch [30/116], Loss: 0.0562, Acc: 95.73%\n",
      "Epoch [5/30], Batch [40/116], Loss: 0.0115, Acc: 95.70%\n",
      "Epoch [5/30], Batch [50/116], Loss: 0.0425, Acc: 96.06%\n",
      "Epoch [5/30], Batch [60/116], Loss: 0.1085, Acc: 96.20%\n",
      "Epoch [5/30], Batch [70/116], Loss: 0.0258, Acc: 96.34%\n",
      "Epoch [5/30], Batch [80/116], Loss: 0.0292, Acc: 96.45%\n",
      "Epoch [5/30], Batch [90/116], Loss: 0.0196, Acc: 96.74%\n",
      "Epoch [5/30], Batch [100/116], Loss: 0.0871, Acc: 96.59%\n",
      "Epoch [5/30], Batch [110/116], Loss: 0.0570, Acc: 96.73%\n",
      "Epoch [5/30], Train Loss: 0.0896, Train Acc: 96.70%, Val Loss: 0.2799, Val Acc: 89.77%, Time: 72.95s\n",
      "Epoch [6/30], Batch [10/116], Loss: 0.0208, Acc: 95.94%\n",
      "Epoch [6/30], Batch [20/116], Loss: 0.0182, Acc: 95.78%\n",
      "Epoch [6/30], Batch [30/116], Loss: 0.4459, Acc: 95.10%\n",
      "Epoch [6/30], Batch [40/116], Loss: 0.0429, Acc: 95.08%\n",
      "Epoch [6/30], Batch [50/116], Loss: 0.0233, Acc: 95.75%\n",
      "Epoch [6/30], Batch [60/116], Loss: 0.0620, Acc: 96.25%\n",
      "Epoch [6/30], Batch [70/116], Loss: 0.0231, Acc: 96.52%\n",
      "Epoch [6/30], Batch [80/116], Loss: 0.0456, Acc: 96.76%\n",
      "Epoch [6/30], Batch [90/116], Loss: 0.0665, Acc: 96.94%\n",
      "Epoch [6/30], Batch [100/116], Loss: 0.0562, Acc: 97.16%\n",
      "Epoch [6/30], Batch [110/116], Loss: 0.0449, Acc: 97.27%\n",
      "Epoch [6/30], Train Loss: 0.0784, Train Acc: 97.38%, Val Loss: 0.1196, Val Acc: 96.02%, Time: 73.08s\n",
      "Epoch [7/30], Batch [10/116], Loss: 0.0191, Acc: 99.06%\n",
      "Epoch [7/30], Batch [20/116], Loss: 0.0416, Acc: 99.06%\n",
      "Epoch [7/30], Batch [30/116], Loss: 0.2419, Acc: 98.75%\n",
      "Epoch [7/30], Batch [40/116], Loss: 0.0123, Acc: 98.36%\n",
      "Epoch [7/30], Batch [50/116], Loss: 0.0090, Acc: 98.50%\n",
      "Epoch [7/30], Batch [60/116], Loss: 0.0192, Acc: 98.33%\n",
      "Epoch [7/30], Batch [70/116], Loss: 0.0406, Acc: 98.21%\n",
      "Epoch [7/30], Batch [80/116], Loss: 0.0280, Acc: 98.28%\n",
      "Epoch [7/30], Batch [90/116], Loss: 0.0632, Acc: 98.33%\n",
      "Epoch [7/30], Batch [100/116], Loss: 0.0287, Acc: 98.44%\n",
      "Epoch [7/30], Batch [110/116], Loss: 0.0935, Acc: 98.41%\n",
      "Epoch [7/30], Train Loss: 0.0499, Train Acc: 98.43%, Val Loss: 0.2104, Val Acc: 92.99%, Time: 73.12s\n",
      "Epoch [8/30], Batch [10/116], Loss: 0.0127, Acc: 97.81%\n",
      "Epoch [8/30], Batch [20/116], Loss: 0.0269, Acc: 98.44%\n",
      "Epoch [8/30], Batch [30/116], Loss: 0.0053, Acc: 98.75%\n",
      "Epoch [8/30], Batch [40/116], Loss: 0.0017, Acc: 99.06%\n",
      "Epoch [8/30], Batch [50/116], Loss: 0.0198, Acc: 99.25%\n",
      "Epoch [8/30], Batch [60/116], Loss: 0.0033, Acc: 99.32%\n",
      "Epoch [8/30], Batch [70/116], Loss: 0.0039, Acc: 99.38%\n",
      "Epoch [8/30], Batch [80/116], Loss: 0.0013, Acc: 99.45%\n",
      "Epoch [8/30], Batch [90/116], Loss: 0.0031, Acc: 99.48%\n",
      "Epoch [8/30], Batch [100/116], Loss: 0.0026, Acc: 99.50%\n",
      "Epoch [8/30], Batch [110/116], Loss: 0.0048, Acc: 99.55%\n",
      "Epoch [8/30], Train Loss: 0.0182, Train Acc: 99.57%, Val Loss: 0.0588, Val Acc: 97.92%, Time: 73.18s\n",
      "Epoch [9/30], Batch [10/116], Loss: 0.1443, Acc: 99.69%\n",
      "Epoch [9/30], Batch [20/116], Loss: 0.0624, Acc: 99.22%\n",
      "Epoch [9/30], Batch [30/116], Loss: 0.0050, Acc: 99.27%\n",
      "Epoch [9/30], Batch [40/116], Loss: 0.0149, Acc: 99.38%\n",
      "Epoch [9/30], Batch [50/116], Loss: 0.0013, Acc: 99.50%\n",
      "Epoch [9/30], Batch [60/116], Loss: 0.0063, Acc: 99.53%\n",
      "Epoch [9/30], Batch [70/116], Loss: 0.0079, Acc: 99.51%\n",
      "Epoch [9/30], Batch [80/116], Loss: 0.0025, Acc: 99.49%\n",
      "Epoch [9/30], Batch [90/116], Loss: 0.0018, Acc: 99.48%\n",
      "Epoch [9/30], Batch [100/116], Loss: 0.0018, Acc: 99.53%\n",
      "Epoch [9/30], Batch [110/116], Loss: 0.0014, Acc: 99.57%\n",
      "Epoch [9/30], Train Loss: 0.0132, Train Acc: 99.59%, Val Loss: 0.0839, Val Acc: 98.11%, Time: 73.17s\n",
      "Epoch [10/30], Batch [10/116], Loss: 0.0011, Acc: 100.00%\n",
      "Epoch [10/30], Batch [20/116], Loss: 0.0014, Acc: 100.00%\n",
      "Epoch [10/30], Batch [30/116], Loss: 0.0011, Acc: 100.00%\n",
      "Epoch [10/30], Batch [40/116], Loss: 0.0012, Acc: 100.00%\n",
      "Epoch [10/30], Batch [50/116], Loss: 0.0012, Acc: 100.00%\n",
      "Epoch [10/30], Batch [60/116], Loss: 0.0017, Acc: 100.00%\n",
      "Epoch [10/30], Batch [70/116], Loss: 0.0022, Acc: 100.00%\n",
      "Epoch [10/30], Batch [80/116], Loss: 0.0011, Acc: 100.00%\n",
      "Epoch [10/30], Batch [90/116], Loss: 0.0014, Acc: 100.00%\n",
      "Epoch [10/30], Batch [100/116], Loss: 0.0012, Acc: 100.00%\n",
      "Epoch [10/30], Batch [110/116], Loss: 0.0011, Acc: 100.00%\n",
      "Epoch [10/30], Train Loss: 0.0019, Train Acc: 100.00%, Val Loss: 0.0610, Val Acc: 98.30%, Time: 73.19s\n",
      "Epoch [11/30], Batch [10/116], Loss: 0.0014, Acc: 99.69%\n",
      "Epoch [11/30], Batch [20/116], Loss: 0.0016, Acc: 99.84%\n",
      "Epoch [11/30], Batch [30/116], Loss: 0.0031, Acc: 99.90%\n",
      "Epoch [11/30], Batch [40/116], Loss: 0.0008, Acc: 99.92%\n",
      "Epoch [11/30], Batch [50/116], Loss: 0.0010, Acc: 99.94%\n",
      "Epoch [11/30], Batch [60/116], Loss: 0.0011, Acc: 99.95%\n",
      "Epoch [11/30], Batch [70/116], Loss: 0.0011, Acc: 99.96%\n",
      "Epoch [11/30], Batch [80/116], Loss: 0.0009, Acc: 99.96%\n",
      "Epoch [11/30], Batch [90/116], Loss: 0.0008, Acc: 99.97%\n",
      "Epoch [11/30], Batch [100/116], Loss: 0.0009, Acc: 99.97%\n",
      "Epoch [11/30], Batch [110/116], Loss: 0.0007, Acc: 99.97%\n",
      "Epoch [11/30], Train Loss: 0.0015, Train Acc: 99.97%, Val Loss: 0.0603, Val Acc: 97.35%, Time: 73.20s\n",
      "Epoch [12/30], Batch [10/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [12/30], Batch [20/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [12/30], Batch [30/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [12/30], Batch [40/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [12/30], Batch [50/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [12/30], Batch [60/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [12/30], Batch [70/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [12/30], Batch [80/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [12/30], Batch [90/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [12/30], Batch [100/116], Loss: 0.0009, Acc: 100.00%\n",
      "Epoch [12/30], Batch [110/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [12/30], Train Loss: 0.0008, Train Acc: 100.00%, Val Loss: 0.0598, Val Acc: 97.35%, Time: 73.26s\n",
      "Epoch [13/30], Batch [10/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [13/30], Batch [20/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [13/30], Batch [30/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [13/30], Batch [40/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [13/30], Batch [50/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [13/30], Batch [60/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [13/30], Batch [70/116], Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [13/30], Batch [80/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [13/30], Batch [90/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [13/30], Batch [100/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [13/30], Batch [110/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [13/30], Train Loss: 0.0007, Train Acc: 100.00%, Val Loss: 0.0563, Val Acc: 97.35%, Time: 73.28s\n",
      "Epoch [14/30], Batch [10/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [14/30], Batch [20/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [14/30], Batch [30/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [14/30], Batch [40/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [14/30], Batch [50/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [14/30], Batch [60/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [14/30], Batch [70/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [14/30], Batch [80/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [14/30], Batch [90/116], Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [14/30], Batch [100/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [14/30], Batch [110/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [14/30], Train Loss: 0.0006, Train Acc: 100.00%, Val Loss: 0.0550, Val Acc: 97.35%, Time: 73.39s\n",
      "Epoch [15/30], Batch [10/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [15/30], Batch [20/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [15/30], Batch [30/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [15/30], Batch [40/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [15/30], Batch [50/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [15/30], Batch [60/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [15/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [15/30], Batch [80/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [15/30], Batch [90/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [15/30], Batch [100/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [15/30], Batch [110/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [15/30], Train Loss: 0.0006, Train Acc: 100.00%, Val Loss: 0.0577, Val Acc: 97.35%, Time: 73.39s\n",
      "Epoch [16/30], Batch [10/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [16/30], Batch [20/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [16/30], Batch [30/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [16/30], Batch [40/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [16/30], Batch [50/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [16/30], Batch [60/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [16/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [16/30], Batch [80/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [16/30], Batch [90/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [16/30], Batch [100/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [16/30], Batch [110/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [16/30], Train Loss: 0.0005, Train Acc: 100.00%, Val Loss: 0.0613, Val Acc: 97.35%, Time: 73.42s\n",
      "Epoch [17/30], Batch [10/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [17/30], Batch [20/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [17/30], Batch [30/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [17/30], Batch [40/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [17/30], Batch [50/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [17/30], Batch [60/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [17/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [17/30], Batch [80/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [17/30], Batch [90/116], Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [17/30], Batch [100/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [17/30], Batch [110/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [17/30], Train Loss: 0.0005, Train Acc: 100.00%, Val Loss: 0.0549, Val Acc: 97.35%, Time: 73.38s\n",
      "Epoch [18/30], Batch [10/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [20/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [30/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [40/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [50/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [60/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [18/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [18/30], Batch [100/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Batch [110/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [18/30], Train Loss: 0.0005, Train Acc: 100.00%, Val Loss: 0.0514, Val Acc: 97.35%, Time: 73.41s\n",
      "Epoch [19/30], Batch [10/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [19/30], Batch [20/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [19/30], Batch [30/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [19/30], Batch [40/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [19/30], Batch [50/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [19/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [19/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [19/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [19/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [19/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [19/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [19/30], Train Loss: 0.0005, Train Acc: 100.00%, Val Loss: 0.0536, Val Acc: 97.35%, Time: 73.45s\n",
      "Epoch [20/30], Batch [10/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [20/30], Batch [20/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [20/30], Batch [30/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [20/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [20/30], Batch [50/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [20/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [20/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [20/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [20/30], Batch [90/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [20/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [20/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [20/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0503, Val Acc: 97.35%, Time: 73.47s\n",
      "Epoch [21/30], Batch [10/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [21/30], Batch [20/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [21/30], Batch [30/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [21/30], Batch [40/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [21/30], Batch [50/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [21/30], Batch [60/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [21/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [21/30], Batch [80/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [21/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [21/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [21/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [21/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0501, Val Acc: 97.35%, Time: 73.47s\n",
      "Epoch [22/30], Batch [10/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [22/30], Batch [20/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [22/30], Batch [30/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [22/30], Batch [40/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [22/30], Batch [50/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [22/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [22/30], Batch [70/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [22/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [22/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [22/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [22/30], Batch [110/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [22/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0497, Val Acc: 97.35%, Time: 73.46s\n",
      "Epoch [23/30], Batch [10/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Batch [20/116], Loss: 0.0003, Acc: 100.00%\n",
      "Epoch [23/30], Batch [30/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Batch [50/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Batch [70/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Batch [100/116], Loss: 0.0003, Acc: 100.00%\n",
      "Epoch [23/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [23/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0494, Val Acc: 97.54%, Time: 73.49s\n",
      "Epoch [24/30], Batch [10/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [24/30], Batch [20/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [24/30], Batch [30/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [24/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [24/30], Batch [50/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [24/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [24/30], Batch [70/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [24/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [24/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [24/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [24/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [24/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0495, Val Acc: 97.54%, Time: 73.46s\n",
      "Epoch [25/30], Batch [10/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [20/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [30/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [50/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [70/116], Loss: 0.0003, Acc: 100.00%\n",
      "Epoch [25/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [25/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0544, Val Acc: 97.54%, Time: 73.44s\n",
      "Epoch [26/30], Batch [10/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [20/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [30/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [50/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [70/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [26/30], Batch [110/116], Loss: 0.0003, Acc: 100.00%\n",
      "Epoch [26/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0490, Val Acc: 97.54%, Time: 73.45s\n",
      "Epoch [27/30], Batch [10/116], Loss: 0.0003, Acc: 100.00%\n",
      "Epoch [27/30], Batch [20/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [30/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [50/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [60/116], Loss: 0.0003, Acc: 100.00%\n",
      "Epoch [27/30], Batch [70/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Batch [100/116], Loss: 0.0003, Acc: 100.00%\n",
      "Epoch [27/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [27/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0489, Val Acc: 97.54%, Time: 73.45s\n",
      "Epoch [28/30], Batch [10/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [28/30], Batch [20/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [28/30], Batch [30/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [28/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [28/30], Batch [50/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [28/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [28/30], Batch [70/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [28/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [28/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [28/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [28/30], Batch [110/116], Loss: 0.0003, Acc: 100.00%\n",
      "Epoch [28/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0489, Val Acc: 97.54%, Time: 73.46s\n",
      "Epoch [29/30], Batch [10/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [20/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [30/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [40/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [50/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [70/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [29/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0525, Val Acc: 97.54%, Time: 73.46s\n",
      "Epoch [30/30], Batch [10/116], Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [30/30], Batch [20/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Batch [30/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Batch [40/116], Loss: 0.0003, Acc: 100.00%\n",
      "Epoch [30/30], Batch [50/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Batch [60/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Batch [70/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Batch [80/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Batch [90/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Batch [100/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Batch [110/116], Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [30/30], Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0488, Val Acc: 97.54%, Time: 73.47s\n",
      "Test Accuracy: 97.25%\n",
      "Ambulance Left Accuracy: 100.00%\n",
      "Ambulance Middle Accuracy: 100.00%\n",
      "Ambulance Right Accuracy: 96.91%\n",
      "Car Horn Left Accuracy: 96.67%\n",
      "Car Horn Middle Accuracy: 97.75%\n",
      "Car Horn Right Accuracy: 98.88%\n",
      "Fire Truck Left Accuracy: 98.15%\n",
      "Fire Truck Middle Accuracy: 93.44%\n",
      "Fire Truck Right Accuracy: 96.55%\n",
      "Police Car Left Accuracy: 98.26%\n",
      "Police Car Middle Accuracy: 98.92%\n",
      "Police Car Right Accuracy: 90.10%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import time\n",
    "\n",
    "class DirectionalSoundDataset(Dataset):\n",
    "    def __init__(self, base_dir, transform=None, target_size=(224, 224)):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Define class mapping for all vehicle types and directions\n",
    "        self.class_to_idx = {\n",
    "            'ambulance_L': 0,\n",
    "            'ambulance_M': 1,\n",
    "            'ambulance_R': 2,\n",
    "            'carhorns_L': 3,\n",
    "            'carhorns_M': 4,\n",
    "            'carhorns_R': 5,\n",
    "            'FireTruck_L': 6,\n",
    "            'FireTruck_M': 7,\n",
    "            'FireTruck_R': 8,\n",
    "            'policecar_L': 9,\n",
    "            'policecar_M': 10,\n",
    "            'policecar_R': 11\n",
    "        }\n",
    "        \n",
    "        # Collect all files and their labels\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name in self.class_to_idx.keys():\n",
    "            class_dir = self.base_dir / class_name\n",
    "            if class_dir.exists():\n",
    "                class_files = list(class_dir.glob(f\"{class_name}_*.png\"))\n",
    "                self.files.extend(class_files)\n",
    "                self.labels.extend([self.class_to_idx[class_name]] * len(class_files))\n",
    "        \n",
    "        if len(self.files) == 0:\n",
    "            raise RuntimeError(f\"No spectrogram files found in {base_dir}\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load and process image\n",
    "        spectrogram = Image.open(img_path).convert('RGB')\n",
    "        if spectrogram.size != self.target_size:\n",
    "            spectrogram = spectrogram.resize(self.target_size)\n",
    "        \n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        return spectrogram, label\n",
    "\n",
    "def create_data_loaders(base_dir, batch_size=32, test_size=0.2, val_size=0.1):\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Create full dataset\n",
    "    full_dataset = DirectionalSoundDataset(\n",
    "        base_dir=base_dir,\n",
    "        transform=transform,\n",
    "        target_size=(224, 224)\n",
    "    )\n",
    "    \n",
    "    # Calculate sizes\n",
    "    total_size = len(full_dataset)\n",
    "    indices = list(range(total_size))\n",
    "    \n",
    "    # First split into train and test\n",
    "    train_idx, test_idx = train_test_split(indices, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Then split train into train and val\n",
    "    train_idx, val_idx = train_test_split(train_idx, test_size=val_size/(1-test_size), random_state=42)\n",
    "    \n",
    "    # Create samplers\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    # Create data loaders with reduced num_workers to prevent memory issues\n",
    "    train_loader = DataLoader(\n",
    "        full_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        full_dataset, batch_size=batch_size, sampler=val_sampler,\n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        full_dataset, batch_size=batch_size, sampler=test_sampler,\n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "class DirectionalSoundViT(nn.Module):\n",
    "    def __init__(self, num_classes=12):  # Updated to 12 classes\n",
    "        super().__init__()\n",
    "        self.vit = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        num_features = self.vit.heads.head.in_features\n",
    "        self.vit.heads.head = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vit(x)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=30):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create directory for saving checkpoints\n",
    "    save_dir = Path('model_checkpoints')\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
    "                try:\n",
    "                    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                    \n",
    "                    outputs = model(spectrograms)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    \n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "                    if (batch_idx + 1) % 10 == 0:\n",
    "                        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                              f'Batch [{batch_idx+1}/{len(train_loader)}], '\n",
    "                              f'Loss: {loss.item():.4f}, '\n",
    "                              f'Acc: {100.*correct/total:.2f}%')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            train_acc = 100.*correct/total\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            epoch_time = time.time() - start_time\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for spectrograms, labels in val_loader:\n",
    "                    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                    outputs = model(spectrograms)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            val_acc = 100.*correct/total\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, '\n",
    "                  f'Time: {epoch_time:.2f}s')\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                }, save_dir / 'best_model.pth')\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for spectrograms, labels in test_loader:\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            outputs = model(spectrograms)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    # Print per-class accuracy\n",
    "    class_names = [\n",
    "        'Ambulance Left', 'Ambulance Middle', 'Ambulance Right',\n",
    "        'Car Horn Left', 'Car Horn Middle', 'Car Horn Right',\n",
    "        'Fire Truck Left', 'Fire Truck Middle', 'Fire Truck Right',\n",
    "        'Police Car Left', 'Police Car Middle', 'Police Car Right'\n",
    "    ]\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_mask = (all_labels == i)\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_acc = 100 * np.sum((all_predictions == i) & class_mask) / np.sum(class_mask)\n",
    "            print(f'{class_name} Accuracy: {class_acc:.2f}%')\n",
    "    \n",
    "    return accuracy, all_predictions, all_labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the path to your dataset directory\n",
    "    base_dir = \"Dataset of warning sound types and source directions\"\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(base_dir, batch_size=32)\n",
    "    \n",
    "    # Create model with 12 classes\n",
    "    model = DirectionalSoundViT(num_classes=12)\n",
    "    \n",
    "    # Train model\n",
    "    train_model(model, train_loader, val_loader, num_epochs=30)\n",
    "    \n",
    "    # Evaluate model\n",
    "    accuracy, predictions, labels = evaluate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
